{
    "docs": [
        {
            "location": "/", 
            "text": "casebase\n: A Statistical Software Tool for Survival Analysis\n\n\n\nThis software is written in the open source software environment \nR\n. It's main functionality is to fit smooth-in-time parametric hazard functions using case-base sampling \nHanley \n&\n Miettinen (2009)\n.\n\n\n\n\n\nThis approach allows the explicit inclusion of the time variable into the model, which enables the user to fit a wide class of parametric hazard functions. For example, including time linearly recovers the Gompertz hazard, whereas including time logarithmically recovers the Weibull hazard; not including time at all corresponds to the exponential hazard.\n\n\n\n\n\nThe theoretical properties of this approach have been studied in \nSaarela \n&\n Arjas (2015) and Saarela (2015)\n.\n\n\n\n\n\nThe software is still in development mode.\n\n\n\n\n\nInstallation\n\n\n\n\n\nThe software package is available on \nGithub\n and can be installed directly from within \nR\n using the following commands (note: you will need the \npacman\n package prior to installing the \ncasebase\n package)\n\n\n\n\nlibrary\n(\npacman\n)\n\npacman\n::\np_install_gh\n(\nsahirbhatnagar/casebase\n)", 
            "title": "Home"
        }, 
        {
            "location": "/#casebase-a-statistical-software-tool-for-survival-analysis", 
            "text": "This software is written in the open source software environment  R . It's main functionality is to fit smooth-in-time parametric hazard functions using case-base sampling  Hanley  &  Miettinen (2009) .  \nThis approach allows the explicit inclusion of the time variable into the model, which enables the user to fit a wide class of parametric hazard functions. For example, including time linearly recovers the Gompertz hazard, whereas including time logarithmically recovers the Weibull hazard; not including time at all corresponds to the exponential hazard.  \nThe theoretical properties of this approach have been studied in  Saarela  &  Arjas (2015) and Saarela (2015) .  \nThe software is still in development mode.", 
            "title": "casebase: A Statistical Software Tool for Survival Analysis"
        }, 
        {
            "location": "/intro/", 
            "text": "casebase: An Alternative Framework for Survival Analysis\n\n\nThis vignette introduces the main functions in the \ncasebase\n package.\nThe methods implemented in this package are based on the method\ndevelopped in \nFitting Smooth-in-Time Prognostic Risk Functions via\nLogistic Regression (Hanley and Miettinen,\n2009)\n.\nA rigorous treatment of the theory is developed in \nA case-base sampling\nmethod for estimating recurrent event intensities (Saarela,\n2015)\n\nand \nNon-parametric Bayesian Hazard Regression for Chronic Disease Risk\nAssessment\n.\nThe motivation for this work is nicely summarised by Cox:\n\n\n\n\nWhy another package for survival analysis?\n\n\nThe purpose of the \ncasebase\n package is to provide practitioners with\nan easy-to-use software tool to predict the risk (or cumulative\nincidence (CI)) of an event, for a particular patient. The following\npoints should be noted:\n\n\n\n\nTime matching/risk set sampling (including Cox partial likelihood)\n    eliminates the baseline hazard from the likelihood expression for\n    the hazard ratios\n\n\nIf, however, the absolute risks are of interest, they have to be\n    recovered using the semi-parametric Breslow estimator\n\n\nAlternative approaches for fitting flexible hazard models for\n    estimating absolute risks, not requiring this two-step approach?\n    Yes! \nHanley and Miettinen,\n    2009\n\n\n\n\n\n\nHanley and Miettinen,\n2009\n\npropose a fully parametric hazard model that can be fit via logistic\nregression. From the fitted hazard function, cumulative incidence and,\nthus, risk functions of time, treatment and profile can be easily\nderived.\n\n\n\n\nParametric family of hazard functions\n\n\nThe \ncasebase\n package fits the family of hazard functions of the form\n\n\n\n\n h(x,t) = exp[g(x,t)] \n where \\( t \\) denotes the numerical value\n(number of units) of a point in prognostic/prospective time and \\( x\n\\) is the realization of the vector \\( X \\) of variates based on the\npatient's profile and intervention (if any). Different functions of \\(\nt \\) lead to different parametric hazard models.\n\n\nThe simplest of these models is the one-parameter exponential\ndistribution which is obtained by taking the hazard function to be\nconstant over the range of \\( t \\).\n\n\n\n\n h(x,t) = exp(\\beta_0 + \\beta_1 x) \n\n\n\n\nThe instantaneous failure rate is independent of \\( t \\), so that the\nconditional chance of failure in a time interval of specified length is\nthe same regardless of how long the individual has been on study a.k.a\nthe memoryless property (Kalbfleisch and Prentice, 2002).\n\n\nThe Gompertz hazard model is given by including a linear term for time:\n\n\n\n\n h(x,t)  = exp(\\beta_0 + \\beta_1 t + \\beta_2 x) \n\n\n\n\nUse of \\( log(t) \\) yields the Weibull hazard which allows for a power\ndependence of the hazard on time (Kalbfleisch and Prentice, 2002):\n\n\n\n\n h(x, t)  = exp(\\beta_0 + \\beta_1 \\log(t) + \\beta_2 x) \n\n\n\n\nRecall that the relative risk model (Cox, 1972)\n\n\n\n\n \\lambda(t;x) = \\lambda_0(t) exp(\\mathbf{X}\\boldsymbol{\\beta})  \n\nwhere \\( \\lambda_0(\\cdot) \\) is an arbitrary unspecified baseline\nhazard function for continous time.\n\n\nCox Model vs. Case-base Sampling\n\n\nIn the following table we provide a comparison between the Cox model and\ncase-base sampling:\n\n\n\n\n\n\n\n\n\nfeature\n\n\n\n\nCox\n\n\n\n\nCase.Base.Sampling\n\n\n\n\n\n\n\n\n\n\n\n\nmodel type\n\n\n\n\nsemi-parametric\n\n\n\n\nfully parametric (logistic/multinomial regression)\n\n\n\n\n\n\n\n\ntime\n\n\n\n\nleft hand side of the equation\n\n\n\n\nright hand side - allows flexible modeling of time\n\n\n\n\n\n\n\n\ncumulative incidence\n\n\n\n\nstep function\n\n\n\n\nsmooth-in-time curve\n\n\n\n\n\n\n\n\nnon-proportional hazards\n\n\n\n\ninteraction of covariates with time\n\n\n\n\ninteraction of covariates with time\n\n\n\n\n\n\n\n\nmodel testing\n\n\n\n\n\n\n\n\nmake use of GLM framework (LRT, AIC, BIC)\n\n\n\n\n\n\n\n\ncompeting risks\n\n\n\n\ndifficult\n\n\n\n\ncause-specific cumulative incidence functions (CIFs) directly obtained\nvia multinomial regression\n\n\n\n\n\n\n\n\nprediction\n\n\n\n\nKaplan-Meier-based\n\n\n\n\nROC, AUC, risk reclassification probabilities\n\n\n\n\n\n\n\n\n\n\nIntuition Behind Casebase sampling\n\n\nSlides from Olli\nSaarela\n\n\nLoad Required Packages\n\n\nWe fist load the required packages:\n\n\nif\n \n(\n!\nrequireNamespace\n(\npacman\n,\n quietly \n=\n \nTRUE\n))\n install.packages\n(\npacman\n)\n\npacman\n::\np_load\n(\nsurvival\n)\n\npacman\n::\np_load\n(\ncasebase\n)\n\npacman\n::\np_load\n(\nsplines\n)\n\n\n\n\n\n\nEuropean Randomized Study of Prostate Cancer Screening Data\n\n\nThroughout this vignette, we make use of the European Randomized Study\nof Prostate Cancer Screening data which ships with the \ncasebase\n\npackage:\n\n\ndata\n(\nERSPC\n)\n\n\nhead\n(\nERSPC\n)\n\n\n\n\n\n\n##   ScrArm Follow.Up.Time DeadOfPrCa\n## 1      1         0.0027          0\n## 2      1         0.0027          0\n## 3      1         0.0027          0\n## 4      0         0.0027          0\n## 5      0         0.0027          0\n## 6      0         0.0027          0\n\n\n\n\n\nERSPC\n$\nScrArm \n-\n \nfactor\n(\nERSPC\n$\nScrArm\n,\n \n                       levels \n=\n \nc\n(\n0\n,\n1\n),\n \n                       labels \n=\n \nc\n(\nControl group\n,\n \nScreening group\n))\n\n\n\n\n\n\nThe results of this study were published by \nSchroder FH, et al. N Engl\nJ Med\n2009\n.\nThere's a really interesting story on how this data was obtained. See\n\nhelp(ERSPC)\n and \nLiu Z, Rich B, Hanley JA, Recovering the raw data\nbehind a non-parametric survival curve. Systematic Reviews\n2014\n\nfor further details.\n\n\nPopulation Time Plot\n\n\nPopulation time plots can be extremely informative graphical displays of\nsurvival data. They should be the first step in your exploratory data\nanalyses. We facilitate this task in the \ncasebase\n package using the\n\npopTime\n function. We first create the necessary dataset for producing\nthe population time plots:\n\n\npt_object \n-\n casebase\n::\npopTime\n(\nERSPC\n,\n event \n=\n \nDeadOfPrCa\n)\n\n\n\n\n\n\n## \nFollow.Up.Time\n will be used as the time variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\nWe can see its contents and its class:\n\n\nhead\n(\npt_object\n)\n\n\n\n\n\n\n##             ScrArm   time event original.time original.event event status\n## 1: Screening group 0.0027     0        0.0027              0     censored\n## 2: Screening group 0.0027     0        0.0027              0     censored\n## 3: Screening group 0.0027     0        0.0027              0     censored\n## 4:   Control group 0.0027     0        0.0027              0     censored\n## 5:   Control group 0.0027     0        0.0027              0     censored\n## 6:   Control group 0.0027     0        0.0027              0     censored\n##    ycoord yc n_available\n## 1: 159893  0           0\n## 2: 159892  0           0\n## 3: 159891  0           0\n## 4: 159890  0           0\n## 5: 159889  0           0\n## 6: 159888  0           0\n\n\n\n\n\nclass\n(\npt_object\n)\n\n\n\n\n\n\n## [1] \npopTime\n    \ndata.table\n \ndata.frame\n\n\n\n\n\n\nThe \ncasebase\n package has a \nplot\n method for objects of class\n\npopTime\n:\n\n\nplot\n(\npt_object\n)\n\n\n\n\n\n\n\n\n\n\nCan you explain the distinct shape of the grey area?\n\n\n\n\nExposure Stratified Population Time Plot\n\n\nWe can also create exposure stratified plots by specifying the\n\nexposure\n argument in the \npopTime\n function:\n\n\npt_object_strat \n-\n casebase\n::\npopTime\n(\nERSPC\n,\n \n                                     event \n=\n \nDeadOfPrCa\n,\n \n                                     exposure \n=\n \nScrArm\n)\n\n\n\n\n\n\n## \nFollow.Up.Time\n will be used as the time variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\nWe can see its contents and its class:\n\n\nhead\n(\npt_object_strat\n)\n\n\n\n\n\n\n## $data\n##                  ScrArm    time event original.time original.event\n##      1:   Control group  0.0027     0        0.0027              0\n##      2:   Control group  0.0027     0        0.0027              0\n##      3:   Control group  0.0027     0        0.0027              0\n##      4:   Control group  0.0027     0        0.0027              0\n##      5:   Control group  0.0137     0        0.0137              0\n##     ---                                                           \n## 159889: Screening group 14.9405     0       14.9405              0\n## 159890: Screening group 14.9405     0       14.9405              0\n## 159891: Screening group 14.9405     0       14.9405              0\n## 159892: Screening group 14.9405     0       14.9405              0\n## 159893: Screening group 14.9405     0       14.9405              0\n##         event status ycoord yc n_available\n##      1:     censored  88232  0           0\n##      2:     censored  88231  0           0\n##      3:     censored  88230  0           0\n##      4:     censored  88229  0           0\n##      5:     censored  88228  0           0\n##     ---                                   \n## 159889:     censored      5  0           0\n## 159890:     censored      4  0           0\n## 159891:     censored      3  0           0\n## 159892:     censored      2  0           0\n## 159893:     censored      1  0           0\n## \n## $exposure\n## [1] \nScrArm\n\n\n\n\n\n\nclass\n(\npt_object_strat\n)\n\n\n\n\n\n\n## [1] \npopTimeExposure\n \nlist\n\n\n\n\n\n\nThe \ncasebase\n package also has a \nplot\n method for objects of class\n\npopTimeExposure\n:\n\n\nplot\n(\npt_object_strat\n)\n\n\n\n\n\n\n\n\nWe can also plot them side-by-side using the \nncol\n argument:\n\n\nplot\n(\npt_object_strat\n,\n ncol \n=\n \n2\n)\n\n\n\n\n\n\n\n\nCox Model\n\n\nWe first fit a Cox model, examine the hazard ratio for the screening\ngroup (relative to the control group), and plot the cumulative incidence\nfunction (CIF).\n\n\ncox_model \n-\n survival\n::\ncoxph\n(\nSurv\n(\nFollow.Up.Time\n,\n DeadOfPrCa\n)\n \n~\n ScrArm\n,\n \n                             data \n=\n ERSPC\n)\n\n\n(\nsum_cox_model \n-\n \nsummary\n(\ncox_model\n))\n\n\n\n\n\n\n## Call:\n## survival::coxph(formula = Surv(Follow.Up.Time, DeadOfPrCa) ~ \n##     ScrArm, data = ERSPC)\n## \n##   n= 159893, number of events= 540 \n## \n##                         coef exp(coef) se(coef)     z Pr(\n|z|)  \n## ScrArmScreening group -0.222     0.801    0.088 -2.52    0.012 *\n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n##                       exp(coef) exp(-coef) lower .95 upper .95\n## ScrArmScreening group     0.801       1.25     0.674     0.952\n## \n## Concordance= 0.519  (se = 0.011 )\n## Rsquare= 0   (max possible= 0.075 )\n## Likelihood ratio test= 6.45  on 1 df,   p=0.0111\n## Wald test            = 6.37  on 1 df,   p=0.0116\n## Score (logrank) test = 6.39  on 1 df,   p=0.0115\n\n\n\n\n\nWe can plot the CIF for each group:\n\n\nnew_data \n-\n \ndata.frame\n(\nScrArm \n=\n \nc\n(\nControl group\n,\n \nScreening group\n),\n\n                       ignore \n=\n \n99\n)\n\n\nplot\n(\nsurvfit\n(\ncox_model\n,\n newdata\n=\nnew_data\n),\n\n     xlab \n=\n \nYears since Randomization\n,\n \n     ylab\n=\nCumulative Incidence\n,\n \n     fun \n=\n \nevent\n,\n\n     xlim \n=\n \nc\n(\n0\n,\n15\n),\n conf.int \n=\n \nF\n,\n col \n=\n \nc\n(\nred\n,\nblue\n),\n \n     main \n=\n \nsprintf\n(\nEstimated Cumulative Incidence (risk) of Death from Prostate \n\n\n                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)\n,\n\n                    sum_cox_model\n$\nconf.int\n[,\nexp(coef)\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nlower .95\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nupper .95\n]))\n\nlegend\n(\ntopleft\n,\n \n       legend \n=\n \nc\n(\nControl group\n,\n \nScreening group\n),\n \n       col \n=\n \nc\n(\nred\n,\nblue\n),\n\n       lty \n=\n \nc\n(\n1\n,\n \n1\n),\n \n       bg \n=\n \ngray90\n)\n\n\n\n\n\n\n\n\nWe compare it to the figure in \nSchroder FH, et al. N Engl J Med\n2009\n\nand see that the plots are very similar, as is the hazard ratio and 95%\nconfidence interval:\n\n\n\n\nCase-base Sampling\n\n\nNext we fit several models using case-base sampling. The models we fit\ndiffer in how we choose to model time.\n\n\nThe \nfitSmoothHazard\n function provides an estimate of the hazard\nfunction \\( h(x, t) \\) is the hazard function, \\( t \\) denotes the\nnumerical value (number of units) of a point in prognostic/prospective\ntime and \\( x \\) is the realization of the vector \\( X \\) of\nvariates based on the patient's profile and intervention (if any).\n\n\n# set the seed for reproducible output\n\n\nset.seed\n(\n1234\n)\n\n\ncasebase_exponential \n-\n casebase\n::\nfitSmoothHazard\n(\nDeadOfPrCa \n~\n ScrArm\n,\n \n                                                  data \n=\n ERSPC\n,\n \n                                                  ratio \n=\n \n100\n,\n \n                                                  type \n=\n \nuniform\n)\n\n\n\n\n\n\n## \nFollow.Up.Time\n will be used as the time variable\n\n\n\n\n\nsummary\n(\ncasebase_exponential\n)\n\n\n\n\n\n\n## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -0.148  -0.148  -0.148  -0.132   3.080  \n## \n## Coefficients:\n##                       Estimate Std. Error z value            Pr(\n|z|)    \n## (Intercept)            -7.7608     0.0557 -139.36 \n0.0000000000000002 ***\n## ScrArmScreening group  -0.2261     0.0884   -2.56               0.011 *  \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6059.0  on 54539  degrees of freedom\n## Residual deviance: 6052.3  on 54538  degrees of freedom\n## AIC: 6056\n## \n## Number of Fisher Scoring iterations: 7\n\n\n\n\n\nexp\n(\ncasebase_exponential\n$\ncoefficients\n[\n2\n])\n\n\n\n\n\n\n## ScrArmScreening group \n##                   0.8\n\n\n\n\n\nexp\n(\nconfint\n(\ncasebase_exponential\n)[\n2\n,])\n\n\n\n\n\n\n## Waiting for profiling to be done...\n\n##  2.5 % 97.5 % \n##   0.67   0.95\n\n\n\n\n\nThe \nabsoluteRisk\n function provides an estimate of the cumulative\nincidence curves for a specific risk profile using the following\nequation:\n\n\n\n\n CI(x, t) = 1 - exp\\left[ - \\int_0^t h(x, u) \\textrm{d}u \\right] \n\n\n\n\nIn the plot below, we overlay the estimated CIF from the casebase\nexponential model on the Cox model CIF:\n\n\nsmooth_risk_exp \n-\n casebase\n::\nabsoluteRisk\n(\nobject \n=\n casebase_exponential\n,\n \n                                          time \n=\n \nseq\n(\n0\n,\n15\n,\n0.1\n),\n \n                                          newdata \n=\n new_data\n)\n\n\nplot\n(\nsurvfit\n(\ncox_model\n,\n newdata\n=\nnew_data\n),\n\n     xlab \n=\n \nYears since Randomization\n,\n \n     ylab\n=\nCumulative Incidence\n,\n \n     fun \n=\n \nevent\n,\n\n     xlim \n=\n \nc\n(\n0\n,\n15\n),\n conf.int \n=\n \nF\n,\n col \n=\n \nc\n(\nred\n,\nblue\n),\n \n     main \n=\n \nsprintf\n(\nEstimated Cumulative Incidence (risk) of Death from Prostate \n\n\n                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)\n,\n\n                    sum_cox_model\n$\nconf.int\n[,\nexp(coef)\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nlower .95\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nupper .95\n]))\n\nlines\n(\nseq\n(\n0\n,\n15\n,\n0.1\n),\n smooth_risk_exp\n[\n1\n,],\n type \n=\n \nl\n,\n col \n=\n \nred\n,\n lty \n=\n \n2\n)\n\nlines\n(\nseq\n(\n0\n,\n15\n,\n0.1\n),\n smooth_risk_exp\n[\n2\n,],\n type \n=\n \nl\n,\n col \n=\n \nblue\n,\n lty \n=\n \n2\n)\n\n\n\nlegend\n(\ntopleft\n,\n \n       legend \n=\n \nc\n(\nControl group (Cox)\n,\nControl group (Casebase)\n,\n\n                  \nScreening group (Cox)\n,\n \nScreening group (Casebase)\n),\n \n       col \n=\n \nc\n(\nred\n,\nred\n,\n \nblue\n,\nblue\n),\n\n       lty \n=\n \nc\n(\n1\n,\n \n2\n,\n \n1\n,\n \n2\n),\n \n       bg \n=\n \ngray90\n)\n\n\n\n\n\n\n\n\nAs we can see, the exponential model is not a good fit. Based on what we\nobserved in the population time plot, where more events are observed\nlater on in time, this poor fit is expected. A constant hazard model\nwould overestimate the cumulative incidence earlier on in time, and\nunderestimate it later on, which is what we see in the cumulative\nincidence plot. This example demonstrates the benefits of population\ntime plots as an exploratory analysis tool.\n\n\nLinear Time\n\n\nNext we enter time linearly into the model:\n\n\ncasebase_time \n-\n fitSmoothHazard\n(\nDeadOfPrCa \n~\n Follow.Up.Time \n+\n ScrArm\n,\n \n                                 data \n=\n ERSPC\n,\n \n                                 ratio \n=\n \n100\n,\n \n                                 type \n=\n \nuniform\n)\n\n\n\n\n\n\n## \nFollow.Up.Time\n will be used as the time variable\n\n\n\n\n\nsummary\n(\ncasebase_time\n)\n\n\n\n\n\n\n## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -0.391  -0.161  -0.123  -0.095   3.462  \n## \n## Coefficients:\n##                       Estimate Std. Error z value            Pr(\n|z|)    \n## (Intercept)            -9.0013     0.1116  -80.66 \n0.0000000000000002 ***\n## Follow.Up.Time          0.2174     0.0145   15.04 \n0.0000000000000002 ***\n## ScrArmScreening group  -0.2455     0.0886   -2.77              0.0056 ** \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6059.0  on 54539  degrees of freedom\n## Residual deviance: 5818.9  on 54537  degrees of freedom\n## AIC: 5825\n## \n## Number of Fisher Scoring iterations: 8\n\n\n\n\n\nexp\n(\ncasebase_time\n$\ncoefficients\n)\n\n\n\n\n\n\n##           (Intercept)        Follow.Up.Time ScrArmScreening group \n##               0.00012               1.24290               0.78230\n\n\n\n\n\nexp\n(\nconfint\n(\ncasebase_time\n))\n\n\n\n\n\n\n## Waiting for profiling to be done...\n\n##                          2.5 %  97.5 %\n## (Intercept)           0.000099 0.00015\n## Follow.Up.Time        1.208290 1.27878\n## ScrArmScreening group 0.656812 0.92986\n\n\n\n\n\nsmooth_risk_time \n-\n casebase\n::\nabsoluteRisk\n(\nobject \n=\n casebase_time\n,\n \n                                          time \n=\n \nseq\n(\n0\n,\n15\n,\n0.1\n),\n \n                                          newdata \n=\n new_data\n)\n\n\nplot\n(\nsurvfit\n(\ncox_model\n,\n newdata\n=\nnew_data\n),\n\n     xlab \n=\n \nYears since Randomization\n,\n \n     ylab\n=\nCumulative Incidence\n,\n \n     fun \n=\n \nevent\n,\n\n     xlim \n=\n \nc\n(\n0\n,\n15\n),\n conf.int \n=\n \nF\n,\n col \n=\n \nc\n(\nred\n,\nblue\n),\n \n     main \n=\n \nsprintf\n(\nEstimated Cumulative Incidence (risk) of Death from Prostate \n\n\n                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)\n,\n\n                    sum_cox_model\n$\nconf.int\n[,\nexp(coef)\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nlower .95\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nupper .95\n]))\n\nlines\n(\nseq\n(\n0\n,\n15\n,\n0.1\n),\n smooth_risk_time\n[\n1\n,],\n type \n=\n \nl\n,\n col \n=\n \nred\n,\n lty \n=\n \n2\n)\n\nlines\n(\nseq\n(\n0\n,\n15\n,\n0.1\n),\n smooth_risk_time\n[\n2\n,],\n type \n=\n \nl\n,\n col \n=\n \nblue\n,\n lty \n=\n \n2\n)\n\n\nlegend\n(\ntopleft\n,\n \n       legend \n=\n \nc\n(\nControl group (Cox)\n,\nControl group (Casebase)\n,\n\n                  \nScreening group (Cox)\n,\n \nScreening group (Casebase)\n),\n \n       col \n=\n \nc\n(\nred\n,\nred\n,\n \nblue\n,\nblue\n),\n\n       lty \n=\n \nc\n(\n1\n,\n \n2\n,\n \n1\n,\n \n2\n),\n \n       bg \n=\n \ngray90\n)\n\n\n\n\n\n\n\n\nWe see that the Weibull model leads to a better fit.\n\n\nFlexible time using BSplines\n\n\nNext we try to enter a smooth function of time into the model using the\n\nsplines\n package\n\n\ncasebase_splines \n-\n fitSmoothHazard\n(\nDeadOfPrCa \n~\n bs\n(\nFollow.Up.Time\n)\n \n+\n ScrArm\n,\n \n                                    data \n=\n ERSPC\n,\n \n                                    ratio \n=\n \n100\n,\n \n                                    type \n=\n \nuniform\n)\n\n\n\n\n\n\n## \nFollow.Up.Time\n will be used as the time variable\n\n\n\n\n\nsummary\n(\ncasebase_splines\n)\n\n\n\n\n\n\n## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -0.437  -0.173  -0.135  -0.085   3.803  \n## \n## Coefficients:\n##                       Estimate Std. Error z value             Pr(\n|z|)    \n## (Intercept)           -10.3009     0.3182  -32.37 \n 0.0000000000000002 ***\n## bs(Follow.Up.Time)1     4.4289     0.8088    5.48       0.000000043451 ***\n## bs(Follow.Up.Time)2     1.9887     0.4875    4.08       0.000045142561 ***\n## bs(Follow.Up.Time)3     4.7511     0.7215    6.59       0.000000000045 ***\n## ScrArmScreening group  -0.2097     0.0886   -2.37                0.018 *  \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6059.0  on 54539  degrees of freedom\n## Residual deviance: 5789.2  on 54535  degrees of freedom\n## AIC: 5799\n## \n## Number of Fisher Scoring iterations: 8\n\n\n\n\n\nexp\n(\ncasebase_splines\n$\ncoefficients\n)\n\n\n\n\n\n\n##           (Intercept)   bs(Follow.Up.Time)1   bs(Follow.Up.Time)2 \n##              0.000034             83.840791              7.305938 \n##   bs(Follow.Up.Time)3 ScrArmScreening group \n##            115.710908              0.810854\n\n\n\n\n\nexp\n(\nconfint\n(\ncasebase_splines\n))\n\n\n\n\n\n\n## Waiting for profiling to be done...\n\n##                           2.5 %     97.5 %\n## (Intercept)            0.000017   0.000061\n## bs(Follow.Up.Time)1   17.745271 424.854594\n## bs(Follow.Up.Time)2    2.900215  19.704298\n## bs(Follow.Up.Time)3   26.568668 455.040311\n## ScrArmScreening group  0.680804   0.963786\n\n\n\n\n\nsmooth_risk_splines \n-\n absoluteRisk\n(\nobject \n=\n casebase_splines\n,\n \n                                    time \n=\n \nseq\n(\n0\n,\n15\n,\n0.1\n),\n \n                                    newdata \n=\n new_data\n)\n\n\nplot\n(\nsurvfit\n(\ncox_model\n,\n newdata\n=\nnew_data\n),\n\n     xlab \n=\n \nYears since Randomization\n,\n \n     ylab\n=\nCumulative Incidence\n,\n \n     fun \n=\n \nevent\n,\n\n     xlim \n=\n \nc\n(\n0\n,\n15\n),\n conf.int \n=\n \nF\n,\n col \n=\n \nc\n(\nred\n,\nblue\n),\n \n     main \n=\n \nsprintf\n(\nEstimated Cumulative Incidence (risk) of Death from Prostate \n\n\n                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)\n,\n\n                    sum_cox_model\n$\nconf.int\n[,\nexp(coef)\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nlower .95\n],\n \n                    sum_cox_model\n$\nconf.int\n[,\nupper .95\n]))\n\nlines\n(\nseq\n(\n0\n,\n15\n,\n0.1\n),\n smooth_risk_splines\n[\n1\n,],\n type \n=\n \nl\n,\n col \n=\n \nred\n,\n lty \n=\n \n2\n)\n\nlines\n(\nseq\n(\n0\n,\n15\n,\n0.1\n),\n smooth_risk_splines\n[\n2\n,],\n type \n=\n \nl\n,\n col \n=\n \nblue\n,\n lty \n=\n \n2\n)\n\n\nlegend\n(\ntopleft\n,\n \n       legend \n=\n \nc\n(\nControl group (Cox)\n,\nControl group (Casebase)\n,\n\n                  \nScreening group (Cox)\n,\n \nScreening group (Casebase)\n),\n \n       col \n=\n \nc\n(\nred\n,\nred\n,\n \nblue\n,\nblue\n),\n\n       lty \n=\n \nc\n(\n1\n,\n \n2\n,\n \n1\n,\n \n2\n),\n \n       bg \n=\n \ngray90\n)\n\n\n\n\n\n\n\n\nIt looks like the best fit.\n\n\nComparing Models Using Likelihood Ratio Test\n\n\nSince we are in the GLM framework, we can easily test for which model\nbetter fits the data using a Likelihood ratio test (LRT). The null\nhypothesis here is that the linear model is just as good as the larger\n(in terms of number of parameters) splines model.\n\n\nanova\n(\ncasebase_time\n,\n casebase_splines\n,\n test \n=\n \nLRT\n)\n\n\n\n\n\n\n## Analysis of Deviance Table\n## \n## Model 1: DeadOfPrCa ~ Follow.Up.Time + ScrArm + offset(offset)\n## Model 2: DeadOfPrCa ~ bs(Follow.Up.Time) + ScrArm + offset(offset)\n##   Resid. Df Resid. Dev Df Deviance   Pr(\nChi)    \n## 1     54537       5819                           \n## 2     54535       5789  2     29.7 0.00000036 ***\n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n\n\n\n\n\nWe see that splines model is the better fit.\n\n\nSession Information\n\n\nprint\n(\nsessionInfo\n(),\n locale \n=\n \nF\n)\n\n\n\n\n\n\n## R version 3.3.2 (2016-10-31)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.04.1 LTS\n## \n## attached base packages:\n## [1] splines   stats     graphics  grDevices utils     datasets  methods  \n## [8] base     \n## \n## other attached packages:\n## [1] casebase_0.1.0  survival_2.40-1\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.10       knitr_1.15.1       magrittr_1.5      \n##  [4] MASS_7.3-45        munsell_0.4.3      colorspace_1.3-2  \n##  [7] lattice_0.20-34    stringr_1.2.0      plyr_1.8.4        \n## [10] tools_3.3.2        grid_3.3.2         data.table_1.10.4 \n## [13] gtable_0.2.0       pacman_0.4.1       htmltools_0.3.6   \n## [16] assertthat_0.1     yaml_2.1.14        lazyeval_0.2.0    \n## [19] rprojroot_1.2      digest_0.6.12      tibble_1.2        \n## [22] Matrix_1.2-7.1     ggplot2_2.2.1      VGAM_1.0-3        \n## [25] evaluate_0.10      rmarkdown_1.3.9003 labeling_0.3      \n## [28] stringi_1.1.3      scales_0.4.1       backports_1.0.5   \n## [31] stats4_3.3.2", 
            "title": "Introduction"
        }, 
        {
            "location": "/intro/#casebase-an-alternative-framework-for-survival-analysis", 
            "text": "This vignette introduces the main functions in the  casebase  package.\nThe methods implemented in this package are based on the method\ndevelopped in  Fitting Smooth-in-Time Prognostic Risk Functions via\nLogistic Regression (Hanley and Miettinen,\n2009) .\nA rigorous treatment of the theory is developed in  A case-base sampling\nmethod for estimating recurrent event intensities (Saarela,\n2015) \nand  Non-parametric Bayesian Hazard Regression for Chronic Disease Risk\nAssessment .\nThe motivation for this work is nicely summarised by Cox:", 
            "title": "casebase: An Alternative Framework for Survival Analysis"
        }, 
        {
            "location": "/intro/#why-another-package-for-survival-analysis", 
            "text": "The purpose of the  casebase  package is to provide practitioners with\nan easy-to-use software tool to predict the risk (or cumulative\nincidence (CI)) of an event, for a particular patient. The following\npoints should be noted:   Time matching/risk set sampling (including Cox partial likelihood)\n    eliminates the baseline hazard from the likelihood expression for\n    the hazard ratios  If, however, the absolute risks are of interest, they have to be\n    recovered using the semi-parametric Breslow estimator  Alternative approaches for fitting flexible hazard models for\n    estimating absolute risks, not requiring this two-step approach?\n    Yes!  Hanley and Miettinen,\n    2009    Hanley and Miettinen,\n2009 \npropose a fully parametric hazard model that can be fit via logistic\nregression. From the fitted hazard function, cumulative incidence and,\nthus, risk functions of time, treatment and profile can be easily\nderived.", 
            "title": "Why another package for survival analysis?"
        }, 
        {
            "location": "/intro/#parametric-family-of-hazard-functions", 
            "text": "The  casebase  package fits the family of hazard functions of the form    h(x,t) = exp[g(x,t)]   where \\( t \\) denotes the numerical value\n(number of units) of a point in prognostic/prospective time and \\( x\n\\) is the realization of the vector \\( X \\) of variates based on the\npatient's profile and intervention (if any). Different functions of \\(\nt \\) lead to different parametric hazard models.  The simplest of these models is the one-parameter exponential\ndistribution which is obtained by taking the hazard function to be\nconstant over the range of \\( t \\).    h(x,t) = exp(\\beta_0 + \\beta_1 x)    The instantaneous failure rate is independent of \\( t \\), so that the\nconditional chance of failure in a time interval of specified length is\nthe same regardless of how long the individual has been on study a.k.a\nthe memoryless property (Kalbfleisch and Prentice, 2002).  The Gompertz hazard model is given by including a linear term for time:    h(x,t)  = exp(\\beta_0 + \\beta_1 t + \\beta_2 x)    Use of \\( log(t) \\) yields the Weibull hazard which allows for a power\ndependence of the hazard on time (Kalbfleisch and Prentice, 2002):    h(x, t)  = exp(\\beta_0 + \\beta_1 \\log(t) + \\beta_2 x)    Recall that the relative risk model (Cox, 1972)    \\lambda(t;x) = \\lambda_0(t) exp(\\mathbf{X}\\boldsymbol{\\beta})   \nwhere \\( \\lambda_0(\\cdot) \\) is an arbitrary unspecified baseline\nhazard function for continous time.", 
            "title": "Parametric family of hazard functions"
        }, 
        {
            "location": "/intro/#cox-model-vs-case-base-sampling", 
            "text": "In the following table we provide a comparison between the Cox model and\ncase-base sampling:     \nfeature  \nCox  \nCase.Base.Sampling      \nmodel type  \nsemi-parametric  \nfully parametric (logistic/multinomial regression)    \ntime  \nleft hand side of the equation  \nright hand side - allows flexible modeling of time    \ncumulative incidence  \nstep function  \nsmooth-in-time curve    \nnon-proportional hazards  \ninteraction of covariates with time  \ninteraction of covariates with time    \nmodel testing    \nmake use of GLM framework (LRT, AIC, BIC)    \ncompeting risks  \ndifficult  \ncause-specific cumulative incidence functions (CIFs) directly obtained\nvia multinomial regression    \nprediction  \nKaplan-Meier-based  \nROC, AUC, risk reclassification probabilities", 
            "title": "Cox Model vs. Case-base Sampling"
        }, 
        {
            "location": "/intro/#intuition-behind-casebase-sampling", 
            "text": "Slides from Olli\nSaarela", 
            "title": "Intuition Behind Casebase sampling"
        }, 
        {
            "location": "/intro/#load-required-packages", 
            "text": "We fist load the required packages:  if   ( ! requireNamespace ( pacman ,  quietly  =   TRUE ))  install.packages ( pacman ) \npacman :: p_load ( survival ) \npacman :: p_load ( casebase ) \npacman :: p_load ( splines )", 
            "title": "Load Required Packages"
        }, 
        {
            "location": "/intro/#european-randomized-study-of-prostate-cancer-screening-data", 
            "text": "Throughout this vignette, we make use of the European Randomized Study\nof Prostate Cancer Screening data which ships with the  casebase \npackage:  data ( ERSPC )  head ( ERSPC )   ##   ScrArm Follow.Up.Time DeadOfPrCa\n## 1      1         0.0027          0\n## 2      1         0.0027          0\n## 3      1         0.0027          0\n## 4      0         0.0027          0\n## 5      0         0.0027          0\n## 6      0         0.0027          0  ERSPC $ ScrArm  -   factor ( ERSPC $ ScrArm ,  \n                       levels  =   c ( 0 , 1 ),  \n                       labels  =   c ( Control group ,   Screening group ))   The results of this study were published by  Schroder FH, et al. N Engl\nJ Med\n2009 .\nThere's a really interesting story on how this data was obtained. See help(ERSPC)  and  Liu Z, Rich B, Hanley JA, Recovering the raw data\nbehind a non-parametric survival curve. Systematic Reviews\n2014 \nfor further details.", 
            "title": "European Randomized Study of Prostate Cancer Screening Data"
        }, 
        {
            "location": "/intro/#population-time-plot", 
            "text": "Population time plots can be extremely informative graphical displays of\nsurvival data. They should be the first step in your exploratory data\nanalyses. We facilitate this task in the  casebase  package using the popTime  function. We first create the necessary dataset for producing\nthe population time plots:  pt_object  -  casebase :: popTime ( ERSPC ,  event  =   DeadOfPrCa )   ##  Follow.Up.Time  will be used as the time variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  We can see its contents and its class:  head ( pt_object )   ##             ScrArm   time event original.time original.event event status\n## 1: Screening group 0.0027     0        0.0027              0     censored\n## 2: Screening group 0.0027     0        0.0027              0     censored\n## 3: Screening group 0.0027     0        0.0027              0     censored\n## 4:   Control group 0.0027     0        0.0027              0     censored\n## 5:   Control group 0.0027     0        0.0027              0     censored\n## 6:   Control group 0.0027     0        0.0027              0     censored\n##    ycoord yc n_available\n## 1: 159893  0           0\n## 2: 159892  0           0\n## 3: 159891  0           0\n## 4: 159890  0           0\n## 5: 159889  0           0\n## 6: 159888  0           0  class ( pt_object )   ## [1]  popTime      data.table   data.frame   The  casebase  package has a  plot  method for objects of class popTime :  plot ( pt_object )     Can you explain the distinct shape of the grey area?", 
            "title": "Population Time Plot"
        }, 
        {
            "location": "/intro/#exposure-stratified-population-time-plot", 
            "text": "We can also create exposure stratified plots by specifying the exposure  argument in the  popTime  function:  pt_object_strat  -  casebase :: popTime ( ERSPC ,  \n                                     event  =   DeadOfPrCa ,  \n                                     exposure  =   ScrArm )   ##  Follow.Up.Time  will be used as the time variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  We can see its contents and its class:  head ( pt_object_strat )   ## $data\n##                  ScrArm    time event original.time original.event\n##      1:   Control group  0.0027     0        0.0027              0\n##      2:   Control group  0.0027     0        0.0027              0\n##      3:   Control group  0.0027     0        0.0027              0\n##      4:   Control group  0.0027     0        0.0027              0\n##      5:   Control group  0.0137     0        0.0137              0\n##     ---                                                           \n## 159889: Screening group 14.9405     0       14.9405              0\n## 159890: Screening group 14.9405     0       14.9405              0\n## 159891: Screening group 14.9405     0       14.9405              0\n## 159892: Screening group 14.9405     0       14.9405              0\n## 159893: Screening group 14.9405     0       14.9405              0\n##         event status ycoord yc n_available\n##      1:     censored  88232  0           0\n##      2:     censored  88231  0           0\n##      3:     censored  88230  0           0\n##      4:     censored  88229  0           0\n##      5:     censored  88228  0           0\n##     ---                                   \n## 159889:     censored      5  0           0\n## 159890:     censored      4  0           0\n## 159891:     censored      3  0           0\n## 159892:     censored      2  0           0\n## 159893:     censored      1  0           0\n## \n## $exposure\n## [1]  ScrArm   class ( pt_object_strat )   ## [1]  popTimeExposure   list   The  casebase  package also has a  plot  method for objects of class popTimeExposure :  plot ( pt_object_strat )    We can also plot them side-by-side using the  ncol  argument:  plot ( pt_object_strat ,  ncol  =   2 )", 
            "title": "Exposure Stratified Population Time Plot"
        }, 
        {
            "location": "/intro/#cox-model", 
            "text": "We first fit a Cox model, examine the hazard ratio for the screening\ngroup (relative to the control group), and plot the cumulative incidence\nfunction (CIF).  cox_model  -  survival :: coxph ( Surv ( Follow.Up.Time ,  DeadOfPrCa )   ~  ScrArm ,  \n                             data  =  ERSPC )  ( sum_cox_model  -   summary ( cox_model ))   ## Call:\n## survival::coxph(formula = Surv(Follow.Up.Time, DeadOfPrCa) ~ \n##     ScrArm, data = ERSPC)\n## \n##   n= 159893, number of events= 540 \n## \n##                         coef exp(coef) se(coef)     z Pr( |z|)  \n## ScrArmScreening group -0.222     0.801    0.088 -2.52    0.012 *\n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n##                       exp(coef) exp(-coef) lower .95 upper .95\n## ScrArmScreening group     0.801       1.25     0.674     0.952\n## \n## Concordance= 0.519  (se = 0.011 )\n## Rsquare= 0   (max possible= 0.075 )\n## Likelihood ratio test= 6.45  on 1 df,   p=0.0111\n## Wald test            = 6.37  on 1 df,   p=0.0116\n## Score (logrank) test = 6.39  on 1 df,   p=0.0115  We can plot the CIF for each group:  new_data  -   data.frame ( ScrArm  =   c ( Control group ,   Screening group ), \n                       ignore  =   99 ) \n\nplot ( survfit ( cox_model ,  newdata = new_data ), \n     xlab  =   Years since Randomization ,  \n     ylab = Cumulative Incidence ,  \n     fun  =   event , \n     xlim  =   c ( 0 , 15 ),  conf.int  =   F ,  col  =   c ( red , blue ),  \n     main  =   sprintf ( Estimated Cumulative Incidence (risk) of Death from Prostate                       Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g) , \n                    sum_cox_model $ conf.int [, exp(coef) ],  \n                    sum_cox_model $ conf.int [, lower .95 ],  \n                    sum_cox_model $ conf.int [, upper .95 ])) \nlegend ( topleft ,  \n       legend  =   c ( Control group ,   Screening group ),  \n       col  =   c ( red , blue ), \n       lty  =   c ( 1 ,   1 ),  \n       bg  =   gray90 )    We compare it to the figure in  Schroder FH, et al. N Engl J Med\n2009 \nand see that the plots are very similar, as is the hazard ratio and 95%\nconfidence interval:", 
            "title": "Cox Model"
        }, 
        {
            "location": "/intro/#case-base-sampling", 
            "text": "Next we fit several models using case-base sampling. The models we fit\ndiffer in how we choose to model time.  The  fitSmoothHazard  function provides an estimate of the hazard\nfunction \\( h(x, t) \\) is the hazard function, \\( t \\) denotes the\nnumerical value (number of units) of a point in prognostic/prospective\ntime and \\( x \\) is the realization of the vector \\( X \\) of\nvariates based on the patient's profile and intervention (if any).  # set the seed for reproducible output  set.seed ( 1234 ) \n\ncasebase_exponential  -  casebase :: fitSmoothHazard ( DeadOfPrCa  ~  ScrArm ,  \n                                                  data  =  ERSPC ,  \n                                                  ratio  =   100 ,  \n                                                  type  =   uniform )   ##  Follow.Up.Time  will be used as the time variable  summary ( casebase_exponential )   ## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -0.148  -0.148  -0.148  -0.132   3.080  \n## \n## Coefficients:\n##                       Estimate Std. Error z value            Pr( |z|)    \n## (Intercept)            -7.7608     0.0557 -139.36  0.0000000000000002 ***\n## ScrArmScreening group  -0.2261     0.0884   -2.56               0.011 *  \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6059.0  on 54539  degrees of freedom\n## Residual deviance: 6052.3  on 54538  degrees of freedom\n## AIC: 6056\n## \n## Number of Fisher Scoring iterations: 7  exp ( casebase_exponential $ coefficients [ 2 ])   ## ScrArmScreening group \n##                   0.8  exp ( confint ( casebase_exponential )[ 2 ,])   ## Waiting for profiling to be done...\n\n##  2.5 % 97.5 % \n##   0.67   0.95  The  absoluteRisk  function provides an estimate of the cumulative\nincidence curves for a specific risk profile using the following\nequation:    CI(x, t) = 1 - exp\\left[ - \\int_0^t h(x, u) \\textrm{d}u \\right]    In the plot below, we overlay the estimated CIF from the casebase\nexponential model on the Cox model CIF:  smooth_risk_exp  -  casebase :: absoluteRisk ( object  =  casebase_exponential ,  \n                                          time  =   seq ( 0 , 15 , 0.1 ),  \n                                          newdata  =  new_data ) \n\nplot ( survfit ( cox_model ,  newdata = new_data ), \n     xlab  =   Years since Randomization ,  \n     ylab = Cumulative Incidence ,  \n     fun  =   event , \n     xlim  =   c ( 0 , 15 ),  conf.int  =   F ,  col  =   c ( red , blue ),  \n     main  =   sprintf ( Estimated Cumulative Incidence (risk) of Death from Prostate                       Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g) , \n                    sum_cox_model $ conf.int [, exp(coef) ],  \n                    sum_cox_model $ conf.int [, lower .95 ],  \n                    sum_cox_model $ conf.int [, upper .95 ])) \nlines ( seq ( 0 , 15 , 0.1 ),  smooth_risk_exp [ 1 ,],  type  =   l ,  col  =   red ,  lty  =   2 ) \nlines ( seq ( 0 , 15 , 0.1 ),  smooth_risk_exp [ 2 ,],  type  =   l ,  col  =   blue ,  lty  =   2 ) \n\n\nlegend ( topleft ,  \n       legend  =   c ( Control group (Cox) , Control group (Casebase) , \n                   Screening group (Cox) ,   Screening group (Casebase) ),  \n       col  =   c ( red , red ,   blue , blue ), \n       lty  =   c ( 1 ,   2 ,   1 ,   2 ),  \n       bg  =   gray90 )    As we can see, the exponential model is not a good fit. Based on what we\nobserved in the population time plot, where more events are observed\nlater on in time, this poor fit is expected. A constant hazard model\nwould overestimate the cumulative incidence earlier on in time, and\nunderestimate it later on, which is what we see in the cumulative\nincidence plot. This example demonstrates the benefits of population\ntime plots as an exploratory analysis tool.", 
            "title": "Case-base Sampling"
        }, 
        {
            "location": "/intro/#linear-time", 
            "text": "Next we enter time linearly into the model:  casebase_time  -  fitSmoothHazard ( DeadOfPrCa  ~  Follow.Up.Time  +  ScrArm ,  \n                                 data  =  ERSPC ,  \n                                 ratio  =   100 ,  \n                                 type  =   uniform )   ##  Follow.Up.Time  will be used as the time variable  summary ( casebase_time )   ## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -0.391  -0.161  -0.123  -0.095   3.462  \n## \n## Coefficients:\n##                       Estimate Std. Error z value            Pr( |z|)    \n## (Intercept)            -9.0013     0.1116  -80.66  0.0000000000000002 ***\n## Follow.Up.Time          0.2174     0.0145   15.04  0.0000000000000002 ***\n## ScrArmScreening group  -0.2455     0.0886   -2.77              0.0056 ** \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6059.0  on 54539  degrees of freedom\n## Residual deviance: 5818.9  on 54537  degrees of freedom\n## AIC: 5825\n## \n## Number of Fisher Scoring iterations: 8  exp ( casebase_time $ coefficients )   ##           (Intercept)        Follow.Up.Time ScrArmScreening group \n##               0.00012               1.24290               0.78230  exp ( confint ( casebase_time ))   ## Waiting for profiling to be done...\n\n##                          2.5 %  97.5 %\n## (Intercept)           0.000099 0.00015\n## Follow.Up.Time        1.208290 1.27878\n## ScrArmScreening group 0.656812 0.92986  smooth_risk_time  -  casebase :: absoluteRisk ( object  =  casebase_time ,  \n                                          time  =   seq ( 0 , 15 , 0.1 ),  \n                                          newdata  =  new_data ) \n\nplot ( survfit ( cox_model ,  newdata = new_data ), \n     xlab  =   Years since Randomization ,  \n     ylab = Cumulative Incidence ,  \n     fun  =   event , \n     xlim  =   c ( 0 , 15 ),  conf.int  =   F ,  col  =   c ( red , blue ),  \n     main  =   sprintf ( Estimated Cumulative Incidence (risk) of Death from Prostate                       Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g) , \n                    sum_cox_model $ conf.int [, exp(coef) ],  \n                    sum_cox_model $ conf.int [, lower .95 ],  \n                    sum_cox_model $ conf.int [, upper .95 ])) \nlines ( seq ( 0 , 15 , 0.1 ),  smooth_risk_time [ 1 ,],  type  =   l ,  col  =   red ,  lty  =   2 ) \nlines ( seq ( 0 , 15 , 0.1 ),  smooth_risk_time [ 2 ,],  type  =   l ,  col  =   blue ,  lty  =   2 ) \n\nlegend ( topleft ,  \n       legend  =   c ( Control group (Cox) , Control group (Casebase) , \n                   Screening group (Cox) ,   Screening group (Casebase) ),  \n       col  =   c ( red , red ,   blue , blue ), \n       lty  =   c ( 1 ,   2 ,   1 ,   2 ),  \n       bg  =   gray90 )    We see that the Weibull model leads to a better fit.", 
            "title": "Linear Time"
        }, 
        {
            "location": "/intro/#flexible-time-using-bsplines", 
            "text": "Next we try to enter a smooth function of time into the model using the splines  package  casebase_splines  -  fitSmoothHazard ( DeadOfPrCa  ~  bs ( Follow.Up.Time )   +  ScrArm ,  \n                                    data  =  ERSPC ,  \n                                    ratio  =   100 ,  \n                                    type  =   uniform )   ##  Follow.Up.Time  will be used as the time variable  summary ( casebase_splines )   ## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -0.437  -0.173  -0.135  -0.085   3.803  \n## \n## Coefficients:\n##                       Estimate Std. Error z value             Pr( |z|)    \n## (Intercept)           -10.3009     0.3182  -32.37   0.0000000000000002 ***\n## bs(Follow.Up.Time)1     4.4289     0.8088    5.48       0.000000043451 ***\n## bs(Follow.Up.Time)2     1.9887     0.4875    4.08       0.000045142561 ***\n## bs(Follow.Up.Time)3     4.7511     0.7215    6.59       0.000000000045 ***\n## ScrArmScreening group  -0.2097     0.0886   -2.37                0.018 *  \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 6059.0  on 54539  degrees of freedom\n## Residual deviance: 5789.2  on 54535  degrees of freedom\n## AIC: 5799\n## \n## Number of Fisher Scoring iterations: 8  exp ( casebase_splines $ coefficients )   ##           (Intercept)   bs(Follow.Up.Time)1   bs(Follow.Up.Time)2 \n##              0.000034             83.840791              7.305938 \n##   bs(Follow.Up.Time)3 ScrArmScreening group \n##            115.710908              0.810854  exp ( confint ( casebase_splines ))   ## Waiting for profiling to be done...\n\n##                           2.5 %     97.5 %\n## (Intercept)            0.000017   0.000061\n## bs(Follow.Up.Time)1   17.745271 424.854594\n## bs(Follow.Up.Time)2    2.900215  19.704298\n## bs(Follow.Up.Time)3   26.568668 455.040311\n## ScrArmScreening group  0.680804   0.963786  smooth_risk_splines  -  absoluteRisk ( object  =  casebase_splines ,  \n                                    time  =   seq ( 0 , 15 , 0.1 ),  \n                                    newdata  =  new_data ) \n\nplot ( survfit ( cox_model ,  newdata = new_data ), \n     xlab  =   Years since Randomization ,  \n     ylab = Cumulative Incidence ,  \n     fun  =   event , \n     xlim  =   c ( 0 , 15 ),  conf.int  =   F ,  col  =   c ( red , blue ),  \n     main  =   sprintf ( Estimated Cumulative Incidence (risk) of Death from Prostate                       Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g) , \n                    sum_cox_model $ conf.int [, exp(coef) ],  \n                    sum_cox_model $ conf.int [, lower .95 ],  \n                    sum_cox_model $ conf.int [, upper .95 ])) \nlines ( seq ( 0 , 15 , 0.1 ),  smooth_risk_splines [ 1 ,],  type  =   l ,  col  =   red ,  lty  =   2 ) \nlines ( seq ( 0 , 15 , 0.1 ),  smooth_risk_splines [ 2 ,],  type  =   l ,  col  =   blue ,  lty  =   2 ) \n\nlegend ( topleft ,  \n       legend  =   c ( Control group (Cox) , Control group (Casebase) , \n                   Screening group (Cox) ,   Screening group (Casebase) ),  \n       col  =   c ( red , red ,   blue , blue ), \n       lty  =   c ( 1 ,   2 ,   1 ,   2 ),  \n       bg  =   gray90 )    It looks like the best fit.", 
            "title": "Flexible time using BSplines"
        }, 
        {
            "location": "/intro/#comparing-models-using-likelihood-ratio-test", 
            "text": "Since we are in the GLM framework, we can easily test for which model\nbetter fits the data using a Likelihood ratio test (LRT). The null\nhypothesis here is that the linear model is just as good as the larger\n(in terms of number of parameters) splines model.  anova ( casebase_time ,  casebase_splines ,  test  =   LRT )   ## Analysis of Deviance Table\n## \n## Model 1: DeadOfPrCa ~ Follow.Up.Time + ScrArm + offset(offset)\n## Model 2: DeadOfPrCa ~ bs(Follow.Up.Time) + ScrArm + offset(offset)\n##   Resid. Df Resid. Dev Df Deviance   Pr( Chi)    \n## 1     54537       5819                           \n## 2     54535       5789  2     29.7 0.00000036 ***\n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1  We see that splines model is the better fit.", 
            "title": "Comparing Models Using Likelihood Ratio Test"
        }, 
        {
            "location": "/intro/#session-information", 
            "text": "print ( sessionInfo (),  locale  =   F )   ## R version 3.3.2 (2016-10-31)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.04.1 LTS\n## \n## attached base packages:\n## [1] splines   stats     graphics  grDevices utils     datasets  methods  \n## [8] base     \n## \n## other attached packages:\n## [1] casebase_0.1.0  survival_2.40-1\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.10       knitr_1.15.1       magrittr_1.5      \n##  [4] MASS_7.3-45        munsell_0.4.3      colorspace_1.3-2  \n##  [7] lattice_0.20-34    stringr_1.2.0      plyr_1.8.4        \n## [10] tools_3.3.2        grid_3.3.2         data.table_1.10.4 \n## [13] gtable_0.2.0       pacman_0.4.1       htmltools_0.3.6   \n## [16] assertthat_0.1     yaml_2.1.14        lazyeval_0.2.0    \n## [19] rprojroot_1.2      digest_0.6.12      tibble_1.2        \n## [22] Matrix_1.2-7.1     ggplot2_2.2.1      VGAM_1.0-3        \n## [25] evaluate_0.10      rmarkdown_1.3.9003 labeling_0.3      \n## [28] stringi_1.1.3      scales_0.4.1       backports_1.0.5   \n## [31] stats4_3.3.2", 
            "title": "Session Information"
        }, 
        {
            "location": "/smoothHazard/", 
            "text": "Methodological details\n\n\nCase-base sampling was proposed by \nHanley and Miettinen,\n2009\n\nas a way to fit smooth-in-time parametric hazard functions via logistic\nregression. The main idea, which was first proposed by Mantel, 1973 and\nthen later developped by Efron, 1977, is to sample person-moments, i.e.\ndiscrete time points along an subject's follow-up time, in order to\nconstruct a base series against which the case series can be compared.\n\n\nThis approach allows the explicit inclusion of the time variable into\nthe model, which enables the user to fit a wide class of parametric\nhazard functions. For example, including time linearly recovers the\nGompertz hazard, whereas including time \nlogarithmically\n recovers the\nWeibull hazard; not including time at all corresponds to the exponential\nhazard.\n\n\nThe theoretical properties of this approach have been studied in\n\nSaarela and Arjas,\n2015\n\nand \nSaarela,\n2015\n.\n\n\nFirst example\n\n\nThe first example we discuss uses the well-known \nveteran\n dataset,\nwhich is part of the \nsurvival\n package. As we can see below, there is\nalmost no censoring, and therefore we can get a good visual\nrepresentation of the survival function:\n\n\nset.seed\n(\n12345\n)\n\n\n\nlibrary\n(\nsurvival\n)\n\ndata\n(\nveteran\n)\n\n\ntable\n(\nveteran\n$\nstatus\n)\n\n\n\n\n\n\n## \n##   0   1 \n##   9 128\n\n\n\n\n\nevtimes \n-\n veteran\n$\ntime\n[\nveteran\n$\nstatus \n==\n \n1\n]\n\nhist\n(\nevtimes\n,\n nclass \n=\n \n30\n,\n main \n=\n \n,\n xlab \n=\n \nSurvival time (days)\n,\n \n     col \n=\n \ngray90\n,\n probability \n=\n \nTRUE\n)\n\ntgrid \n-\n \nseq\n(\n0\n,\n \n1000\n,\n by \n=\n \n10\n)\n\nlines\n(\ntgrid\n,\n dexp\n(\ntgrid\n,\n rate \n=\n \n1.0\n/\nmean\n(\nevtimes\n)),\n \n      lwd \n=\n \n2\n,\n lty \n=\n \n2\n,\n col \n=\n \nred\n)\n\n\n\n\n\n\n\n\nAs we can see, the empirical survival function ressembles an exponential\ndistribution.\n\n\nWe will first try to estimate the hazard function parametrically using\nsome well-known regression routines. But first, we will reformat the\ndata slightly.\n\n\nveteran\n$\nprior \n-\n \nfactor\n(\nveteran\n$\nprior\n,\n levels \n=\n \nc\n(\n0\n,\n \n10\n),\n labels \n=\n \nc\n(\nno\n,\nyes\n))\n\nveteran\n$\ncelltype \n-\n \nfactor\n(\nveteran\n$\ncelltype\n,\n \n                           levels \n=\n \nc\n(\nlarge\n,\n \nsquamous\n,\n \nsmallcell\n,\n \nadeno\n))\n\nveteran\n$\ntrt \n-\n \nfactor\n(\nveteran\n$\ntrt\n,\n levels \n=\n \nc\n(\n1\n,\n \n2\n),\n labels \n=\n \nc\n(\nstandard\n,\n \ntest\n))\n\n\n\n\n\n\nUsing the \neha\n package, we can fit a Weibull form, with different\nvalues of the shape parameter. For \nshape = 1\n, we get an exponential\ndistribution:\n\n\npacman\n::\np_load\n(\neha\n)\n\ny \n-\n \nwith\n(\nveteran\n,\n Surv\n(\ntime\n,\n status\n))\n\n\nmodel1 \n-\n weibreg\n(\ny \n~\n karno \n+\n diagtime \n+\n age \n+\n prior \n+\n celltype \n+\n trt\n,\n \n                  data \n=\n veteran\n,\n shape \n=\n \n1\n)\n\n\nsummary\n(\nmodel1\n)\n\n\n\n\n\n\n## Call:\n## weibreg(formula = y ~ karno + diagtime + age + prior + celltype + \n##     trt, data = veteran, shape = 1)\n## \n## Covariate           Mean       Coef Exp(Coef)  se(Coef)    Wald p\n## karno              68.419    -0.031     0.970     0.005     0.000 \n## diagtime            8.139     0.000     1.000     0.009     0.974 \n## age                57.379    -0.006     0.994     0.009     0.505 \n## prior \n##               no    0.653     0         1           (reference)\n##              yes    0.347     0.049     1.051     0.227     0.827 \n## celltype \n##            large    0.269     0         1           (reference)\n##         squamous    0.421    -0.377     0.686     0.273     0.166 \n##        smallcell    0.206     0.443     1.557     0.261     0.090 \n##            adeno    0.104     0.736     2.087     0.294     0.012 \n## trt \n##         standard    0.477     0         1           (reference)\n##             test    0.523     0.220     1.246     0.199     0.269 \n## \n## log(scale)                    2.811    16.633     0.713     0.000 \n## \n##  Shape is fixed at  1 \n## \n## Events                    128 \n## Total time at risk         16663 \n## Max. log. likelihood      -716.16 \n## LR test statistic         70.1 \n## Degrees of freedom        8 \n## Overall p-value           4.64229e-12\n\n\n\n\n\nIf we take \nshape = 0\n, the shape parameter is estimated along with the\nregression coefficients:\n\n\nmodel2 \n-\n weibreg\n(\ny \n~\n karno \n+\n diagtime \n+\n age \n+\n prior \n+\n celltype \n+\n trt\n,\n \n                  data \n=\n veteran\n,\n shape \n=\n \n0\n)\n\n\nsummary\n(\nmodel2\n)\n\n\n\n\n\n\n## Call:\n## weibreg(formula = y ~ karno + diagtime + age + prior + celltype + \n##     trt, data = veteran, shape = 0)\n## \n## Covariate           Mean       Coef Exp(Coef)  se(Coef)    Wald p\n## karno              68.419    -0.032     0.968     0.005     0.000 \n## diagtime            8.139     0.001     1.001     0.009     0.955 \n## age                57.379    -0.007     0.993     0.009     0.476 \n## prior \n##               no    0.653     0         1           (reference)\n##              yes    0.347     0.047     1.048     0.229     0.836 \n## celltype \n##            large    0.269     0         1           (reference)\n##         squamous    0.421    -0.428     0.651     0.278     0.123 \n##        smallcell    0.206     0.462     1.587     0.262     0.078 \n##            adeno    0.104     0.792     2.208     0.300     0.008 \n## trt \n##         standard    0.477     0         1           (reference)\n##             test    0.523     0.246     1.279     0.203     0.224 \n## \n## log(scale)                    2.864    17.537     0.671     0.000 \n## log(shape)                    0.075     1.077     0.066     0.261 \n## \n## Events                    128 \n## Total time at risk         16663 \n## Max. log. likelihood      -715.55 \n## LR test statistic         65.1 \n## Degrees of freedom        8 \n## Overall p-value           4.65393e-11\n\n\n\n\n\nFinally, we can also fit a Cox proportional hazard:\n\n\nmodel3 \n-\n coxph\n(\ny \n~\n karno \n+\n diagtime \n+\n age \n+\n prior \n+\n celltype \n+\n trt\n,\n \n                data \n=\n veteran\n)\n\n\nsummary\n(\nmodel3\n)\n\n\n\n\n\n\n## Call:\n## coxph(formula = y ~ karno + diagtime + age + prior + celltype + \n##     trt, data = veteran)\n## \n##   n= 137, number of events= 128 \n## \n##                         coef  exp(coef)   se(coef)      z Pr(\n|z|)    \n## karno             -3.282e-02  9.677e-01  5.508e-03 -5.958 2.55e-09 ***\n## diagtime           8.132e-05  1.000e+00  9.136e-03  0.009  0.99290    \n## age               -8.706e-03  9.913e-01  9.300e-03 -0.936  0.34920    \n## prioryes           7.159e-02  1.074e+00  2.323e-01  0.308  0.75794    \n## celltypesquamous  -4.013e-01  6.695e-01  2.827e-01 -1.420  0.15574    \n## celltypesmallcell  4.603e-01  1.584e+00  2.662e-01  1.729  0.08383 .  \n## celltypeadeno      7.948e-01  2.214e+00  3.029e-01  2.624  0.00869 ** \n## trttest            2.946e-01  1.343e+00  2.075e-01  1.419  0.15577    \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n##                   exp(coef) exp(-coef) lower .95 upper .95\n## karno                0.9677     1.0334    0.9573    0.9782\n## diagtime             1.0001     0.9999    0.9823    1.0182\n## age                  0.9913     1.0087    0.9734    1.0096\n## prioryes             1.0742     0.9309    0.6813    1.6937\n## celltypesquamous     0.6695     1.4938    0.3847    1.1651\n## celltypesmallcell    1.5845     0.6311    0.9403    2.6699\n## celltypeadeno        2.2139     0.4517    1.2228    4.0084\n## trttest              1.3426     0.7448    0.8939    2.0166\n## \n## Concordance= 0.736  (se = 0.03 )\n## Rsquare= 0.364   (max possible= 0.999 )\n## Likelihood ratio test= 62.1  on 8 df,   p=1.799e-10\n## Wald test            = 62.37  on 8 df,   p=1.596e-10\n## Score (logrank) test = 66.74  on 8 df,   p=2.186e-11\n\n\n\n\n\nAs we can see, all three models are significant, and they give similar\ninformation: \nkarno\n and \ncelltype\n are significant predictors, both\ntreatment is not.\n\n\nThe method available in this package makes use of \ncase-base sampling\n.\nThat is, person-moments are randomly sampled across the entire follow-up\ntime, with some moments corresponding to cases and others to controls.\nBy sampling person-moments instead of individuals, we can then use\nlogistic regression to fit smooth-in-time parametric hazard functions.\nSee the previous section for more details.\n\n\nFirst, we will look at the follow-up time by using population-time\nplots:\n\n\n# create popTime object\n\npt_veteran \n-\n casebase\n::\npopTime\n(\ndata \n=\n veteran\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n## \nstatus\n will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\nclass\n(\npt_veteran\n)\n\n\n\n\n\n\n## [1] \npopTime\n    \ndata.table\n \ndata.frame\n\n\n\n\n\n\n# plot method for objects of class \npopTime\n\nplot\n(\npt_veteran\n)\n\n\n\n\n\n\n\n\nPopulation-time plots are a useful way of visualizing the total\nfollow-up experience, where individuals appear on the y-axis, and\nfollow-up time on the x-axis; each individual's follow-up time is\nrepresented by a gray line segment. For convenience, we have ordered the\npatients according to their time-to-event, and each event is represented\nby a red dot. The censored observations (of which there is only a few)\ncorrespond to the grey lines which do not end with a red dot.\n\n\nNext, we use case-base sampling to fit a parametric hazard function via\nlogistic regression. First, we will include time as a linear term; as\nnoted above, this corresponds to an Gompertz hazard.\n\n\nlibrary\n(\ncasebase\n)\n\nmodel4 \n-\n fitSmoothHazard\n(\nstatus \n~\n time \n+\n karno \n+\n diagtime \n+\n age \n+\n prior \n+\n\n             celltype \n+\n trt\n,\n data \n=\n veteran\n,\n ratio \n=\n \n100\n,\n type \n=\n \nuniform\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n\n\n\n\nsummary\n(\nmodel4\n)\n\n\n\n\n\n\n## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.6321  -0.1471  -0.1326  -0.1180   3.3378  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr(\n|z|)    \n## (Intercept)       -4.723692   0.680388  -6.943 3.85e-12 ***\n## time               0.004259   0.000580   7.343 2.09e-13 ***\n## karno             -0.010888   0.004952  -2.199   0.0279 *  \n## diagtime           0.005922   0.009123   0.649   0.5163    \n## age                0.004306   0.009273   0.464   0.6424    \n## prioryes          -0.221476   0.230259  -0.962   0.3361    \n## celltypesquamous  -0.441758   0.287345  -1.537   0.1242    \n## celltypesmallcell  0.022139   0.259385   0.085   0.9320    \n## celltypeadeno      0.186398   0.291500   0.639   0.5225    \n## trttest           -0.100691   0.190795  -0.528   0.5977    \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1436.2  on 12927  degrees of freedom\n## Residual deviance: 1394.6  on 12918  degrees of freedom\n## AIC: 1414.6\n## \n## Number of Fisher Scoring iterations: 7\n\n\n\n\n\nSince the output object from \nfitSmoothHazard\n inherits from the \nglm\n\nclass, we see a familiar result when using the function \nsummary\n.\n\n\nThe main purpose of fitting smooth hazard functions is that it is then\nrelatively easy to compute absolute risks. For example, we can use the\nfunction \nabsoluteRisk\n to compute the mean absolute risk at 90 days,\nwhich can then be compared to the empirical measure.\n\n\nabsoluteRisk\n(\nobject \n=\n model4\n,\n time \n=\n \n90\n)\n\n\n\n\n\n\n## [1] 0.4490265\n\n\n\n\n\nftime \n-\n veteran\n$\ntime\n\nmean\n(\nftime \n=\n \n90\n)\n\n\n\n\n\n\n## [1] 0.5547445\n\n\n\n\n\nWe can also fit a Weibull hazard by using a logarithmic term for time:\n\n\nmodel5 \n-\n fitSmoothHazard\n(\nstatus \n~\n \nlog\n(\ntime\n)\n \n+\n karno \n+\n diagtime \n+\n age \n+\n prior \n+\n\n             celltype \n+\n trt\n,\n data \n=\n veteran\n,\n ratio \n=\n \n100\n,\n type \n=\n \nuniform\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n\n\n\n\nsummary\n(\nmodel5\n)\n\n\n\n\n\n\n## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.3669  -0.1625  -0.1255  -0.0916   3.7345  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr(\n|z|)    \n## (Intercept)       -6.025256   0.715645  -8.419  \n 2e-16 ***\n## log(time)          0.638999   0.081654   7.826 5.05e-15 ***\n## karno             -0.021859   0.005402  -4.046 5.21e-05 ***\n## diagtime           0.002051   0.008846   0.232    0.817    \n## age               -0.002489   0.009044  -0.275    0.783    \n## prioryes           0.038160   0.221385   0.172    0.863    \n## celltypesquamous  -0.160837   0.273353  -0.588    0.556    \n## celltypesmallcell  0.325276   0.263553   1.234    0.217    \n## celltypeadeno      0.475734   0.296983   1.602    0.109    \n## trttest            0.115654   0.191121   0.605    0.545    \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1436.2  on 12927  degrees of freedom\n## Residual deviance: 1363.0  on 12918  degrees of freedom\n## AIC: 1383\n## \n## Number of Fisher Scoring iterations: 8\n\n\n\n\n\nWith case-base sampling, it is straightforward to fit a semi-parametric\nhazard function using splines, which can then be used to estimate the\nmean absolute risk.\n\n\n# Fit a spline for time\n\n\nlibrary\n(\nsplines\n)\n\nmodel6 \n-\n fitSmoothHazard\n(\nstatus \n~\n bs\n(\ntime\n)\n \n+\n karno \n+\n diagtime \n+\n age \n+\n prior \n+\n\n             celltype \n+\n trt\n,\n data \n=\n veteran\n,\n ratio \n=\n \n100\n,\n type \n=\n \nuniform\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n\n\n\n\nsummary\n(\nmodel6\n)\n\n\n\n\n\n\n## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.5950  -0.1519  -0.1207  -0.0984   3.4902  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr(\n|z|)    \n## (Intercept)       -4.850575   0.688886  -7.041 1.91e-12 ***\n## bs(time)1          6.624422   1.057781   6.263 3.79e-10 ***\n## bs(time)2         -2.303337   1.859357  -1.239 0.215426    \n## bs(time)3          4.793540   0.954585   5.022 5.12e-07 ***\n## karno             -0.019221   0.005319  -3.614 0.000302 ***\n## diagtime           0.001291   0.009591   0.135 0.892906    \n## age               -0.002556   0.009120  -0.280 0.779292    \n## prioryes          -0.040716   0.229840  -0.177 0.859390    \n## celltypesquamous  -0.220063   0.281321  -0.782 0.434069    \n## celltypesmallcell  0.365648   0.268621   1.361 0.173449    \n## celltypeadeno      0.542413   0.303971   1.784 0.074355 .  \n## trttest            0.108617   0.197434   0.550 0.582222    \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1436.2  on 12927  degrees of freedom\n## Residual deviance: 1367.3  on 12916  degrees of freedom\n## AIC: 1391.3\n## \n## Number of Fisher Scoring iterations: 8\n\n\n\n\n\nabsoluteRisk\n(\nobject \n=\n model6\n,\n time \n=\n \n90\n)\n\n\n\n\n\n\n## [1] 0.4611582\n\n\n\n\n\nAs we can see from the summary, there is little evidence that splines\nactually improve the fit. Moreover, we can see that estimated individual\nabsolute risks are essentially the same when using either a linear term\nor splines:\n\n\nlinearRisk \n-\n absoluteRisk\n(\nobject \n=\n model4\n,\n time \n=\n \n90\n,\n newdata \n=\n veteran\n)\n\nsplineRisk \n-\n absoluteRisk\n(\nobject \n=\n model6\n,\n time \n=\n \n90\n,\n newdata \n=\n veteran\n)\n\n\nplot\n(\nlinearRisk\n,\n splineRisk\n,\n\n     xlab \n=\n \nLinear\n,\n ylab \n=\n \nSplines\n,\n pch \n=\n \n19\n)\n\nabline\n(\na \n=\n \n0\n,\n b \n=\n \n1\n,\n lty \n=\n \n2\n,\n lwd \n=\n \n2\n,\n col \n=\n \nred\n)\n\n\n\n\n\n\n\n\nThese last three models give similar information as the first three,\ni.e. the main predictors for the hazard are \nkarno\n and \ncelltype\n, with\ntreatment being non-significant. Moreover, by explicitely including the\ntime variable in the formula, we see that it is not significant; this is\nevidence that the true hazard is exponential.\n\n\nFinally, we can look at the estimates of the coefficients for the Cox\nmodel, as well as the last three models (CB stands for \"case-base\"):\n\n\n\n\n\n\n\n\n\n\n\n\n\nCox model\n\n\n\n\nCB linear\n\n\n\n\nCB log-linear\n\n\n\n\nCB splines\n\n\n\n\n\n\n\n\n\n\n\n\nkarno\n\n\n\n\n-0.0328\n\n\n\n\n-0.0109\n\n\n\n\n-0.0219\n\n\n\n\n-0.0192\n\n\n\n\n\n\n\n\ndiagtime\n\n\n\n\n0.0001\n\n\n\n\n0.0059\n\n\n\n\n0.0021\n\n\n\n\n0.0013\n\n\n\n\n\n\n\n\nage\n\n\n\n\n-0.0087\n\n\n\n\n0.0043\n\n\n\n\n-0.0025\n\n\n\n\n-0.0026\n\n\n\n\n\n\n\n\nprioryes\n\n\n\n\n0.0716\n\n\n\n\n-0.2215\n\n\n\n\n0.0382\n\n\n\n\n-0.0407\n\n\n\n\n\n\n\n\ncelltypesquamous\n\n\n\n\n-0.4013\n\n\n\n\n-0.4418\n\n\n\n\n-0.1608\n\n\n\n\n-0.2201\n\n\n\n\n\n\n\n\ncelltypesmallcell\n\n\n\n\n0.4603\n\n\n\n\n0.0221\n\n\n\n\n0.3253\n\n\n\n\n0.3656\n\n\n\n\n\n\n\n\ncelltypeadeno\n\n\n\n\n0.7948\n\n\n\n\n0.1864\n\n\n\n\n0.4757\n\n\n\n\n0.5424\n\n\n\n\n\n\n\n\ntrttest\n\n\n\n\n0.2946\n\n\n\n\n-0.1007\n\n\n\n\n0.1157\n\n\n\n\n0.1086\n\n\n\n\n\n\n\n\n\n\nCumulative Incidence Curves\n\n\nHere we show how to calculate the cumulative incidence curves for a\nspecific risk profile using the following equation:\n\n\n\n\n CI(x, t) = 1 - exp\\left[ - \\int_0^t h(x, u) \\textrm{d}u \\right] \n\nwhere \\( h(x, t) \\) is the hazard function, \\( t \\) denotes the\nnumerical value (number of units) of a point in prognostic/prospective\ntime and \\( x \\) is the realization of the vector \\( X \\) of\nvariates based on the patient's profile and intervention (if any).\n\n\nWe compare the cumulative incidence functions from the fully-parametric\nfit using case base sampling, with those from the Cox model:\n\n\n# define a specific covariate profile\n\nnew_data \n-\n \ndata.frame\n(\ntrt \n=\n \ntest\n,\n \n                       celltype \n=\n \nadeno\n,\n \n                       karno \n=\n median\n(\nveteran\n$\nkarno\n),\n \n                       diagtime \n=\n median\n(\nveteran\n$\ndiagtime\n),\n\n                       age \n=\n median\n(\nveteran\n$\nage\n),\n\n                       prior \n=\n \nno\n)\n\n\n\n# calculate cumulative incidence using casebase model\n\nsmooth_risk \n-\n absoluteRisk\n(\nobject \n=\n model4\n,\n time \n=\n \nseq\n(\n0\n,\n300\n,\n \n1\n),\n \n                            newdata \n=\n new_data\n)\n\n\n\n# cumulative incidence function for the Cox model\n\nplot\n(\nsurvfit\n(\nmodel3\n,\n newdata\n=\nnew_data\n),\n\n     xlab \n=\n \nDays\n,\n ylab\n=\nCumulative Incidence (%)\n,\n fun \n=\n \nevent\n,\n\n     xlim \n=\n \nc\n(\n0\n,\n300\n),\n conf.int \n=\n \nF\n,\n col \n=\n \nred\n,\n \n     main \n=\n \nsprintf\n(\nEstimated Cumulative Incidence (risk) of Lung Cancer\\ntrt = test, celltype = adeno, karno = %g,\\ndiagtime = %g, age = %g, prior = no\n,\n median\n(\nveteran\n$\nkarno\n),\n median\n(\nveteran\n$\ndiagtime\n),\n \n                    median\n(\nveteran\n$\nage\n)))\n\n\n\n# add casebase curve with legend\n\nlines\n(\nseq\n(\n0\n,\n300\n,\n \n1\n),\n smooth_risk\n[\n1\n,],\n type \n=\n \nl\n,\n col \n=\n \nblue\n)\n\nlegend\n(\nbottomright\n,\n \n       legend \n=\n \nc\n(\nsemi-parametric (Cox)\n,\n \nparametric (casebase)\n),\n \n       col \n=\n \nc\n(\nred\n,\nblue\n),\n\n       lty \n=\n \nc\n(\n1\n,\n \n1\n),\n \n       bg \n=\n \ngray90\n)\n\n\n\n\n\n\n\n\nSession information\n\n\n## R version 3.3.1 (2016-06-21)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.10\n## \n## attached base packages:\n## [1] splines   stats     graphics  grDevices utils     datasets  methods  \n## [8] base     \n## \n## other attached packages:\n## [1] casebase_0.1.0  eha_2.4-4       survival_2.39-5\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.9      knitr_1.15.1     magrittr_1.5     munsell_0.4.3   \n##  [5] colorspace_1.3-1 lattice_0.20-33  highr_0.6        plyr_1.8.4      \n##  [9] stringr_1.2.0    tools_3.3.1      grid_3.3.1       data.table_1.9.6\n## [13] gtable_0.2.0     pacman_0.4.1     htmltools_0.3.5  assertthat_0.1  \n## [17] lazyeval_0.2.0   yaml_2.1.14      rprojroot_1.2    digest_0.6.12   \n## [21] tibble_1.2       Matrix_1.2-6     ggplot2_2.2.0    VGAM_1.0-2      \n## [25] evaluate_0.10    rmarkdown_1.3    labeling_0.3     stringi_1.1.2   \n## [29] scales_0.4.1     backports_1.0.5  stats4_3.3.1     chron_2.3-47", 
            "title": "Smooth Hazard"
        }, 
        {
            "location": "/smoothHazard/#methodological-details", 
            "text": "Case-base sampling was proposed by  Hanley and Miettinen,\n2009 \nas a way to fit smooth-in-time parametric hazard functions via logistic\nregression. The main idea, which was first proposed by Mantel, 1973 and\nthen later developped by Efron, 1977, is to sample person-moments, i.e.\ndiscrete time points along an subject's follow-up time, in order to\nconstruct a base series against which the case series can be compared.  This approach allows the explicit inclusion of the time variable into\nthe model, which enables the user to fit a wide class of parametric\nhazard functions. For example, including time linearly recovers the\nGompertz hazard, whereas including time  logarithmically  recovers the\nWeibull hazard; not including time at all corresponds to the exponential\nhazard.  The theoretical properties of this approach have been studied in Saarela and Arjas,\n2015 \nand  Saarela,\n2015 .", 
            "title": "Methodological details"
        }, 
        {
            "location": "/smoothHazard/#first-example", 
            "text": "The first example we discuss uses the well-known  veteran  dataset,\nwhich is part of the  survival  package. As we can see below, there is\nalmost no censoring, and therefore we can get a good visual\nrepresentation of the survival function:  set.seed ( 12345 )  library ( survival ) \ndata ( veteran )  table ( veteran $ status )   ## \n##   0   1 \n##   9 128  evtimes  -  veteran $ time [ veteran $ status  ==   1 ] \nhist ( evtimes ,  nclass  =   30 ,  main  =   ,  xlab  =   Survival time (days) ,  \n     col  =   gray90 ,  probability  =   TRUE ) \ntgrid  -   seq ( 0 ,   1000 ,  by  =   10 ) \nlines ( tgrid ,  dexp ( tgrid ,  rate  =   1.0 / mean ( evtimes )),  \n      lwd  =   2 ,  lty  =   2 ,  col  =   red )    As we can see, the empirical survival function ressembles an exponential\ndistribution.  We will first try to estimate the hazard function parametrically using\nsome well-known regression routines. But first, we will reformat the\ndata slightly.  veteran $ prior  -   factor ( veteran $ prior ,  levels  =   c ( 0 ,   10 ),  labels  =   c ( no , yes )) \nveteran $ celltype  -   factor ( veteran $ celltype ,  \n                           levels  =   c ( large ,   squamous ,   smallcell ,   adeno )) \nveteran $ trt  -   factor ( veteran $ trt ,  levels  =   c ( 1 ,   2 ),  labels  =   c ( standard ,   test ))   Using the  eha  package, we can fit a Weibull form, with different\nvalues of the shape parameter. For  shape = 1 , we get an exponential\ndistribution:  pacman :: p_load ( eha ) \ny  -   with ( veteran ,  Surv ( time ,  status )) \n\nmodel1  -  weibreg ( y  ~  karno  +  diagtime  +  age  +  prior  +  celltype  +  trt ,  \n                  data  =  veteran ,  shape  =   1 )  summary ( model1 )   ## Call:\n## weibreg(formula = y ~ karno + diagtime + age + prior + celltype + \n##     trt, data = veteran, shape = 1)\n## \n## Covariate           Mean       Coef Exp(Coef)  se(Coef)    Wald p\n## karno              68.419    -0.031     0.970     0.005     0.000 \n## diagtime            8.139     0.000     1.000     0.009     0.974 \n## age                57.379    -0.006     0.994     0.009     0.505 \n## prior \n##               no    0.653     0         1           (reference)\n##              yes    0.347     0.049     1.051     0.227     0.827 \n## celltype \n##            large    0.269     0         1           (reference)\n##         squamous    0.421    -0.377     0.686     0.273     0.166 \n##        smallcell    0.206     0.443     1.557     0.261     0.090 \n##            adeno    0.104     0.736     2.087     0.294     0.012 \n## trt \n##         standard    0.477     0         1           (reference)\n##             test    0.523     0.220     1.246     0.199     0.269 \n## \n## log(scale)                    2.811    16.633     0.713     0.000 \n## \n##  Shape is fixed at  1 \n## \n## Events                    128 \n## Total time at risk         16663 \n## Max. log. likelihood      -716.16 \n## LR test statistic         70.1 \n## Degrees of freedom        8 \n## Overall p-value           4.64229e-12  If we take  shape = 0 , the shape parameter is estimated along with the\nregression coefficients:  model2  -  weibreg ( y  ~  karno  +  diagtime  +  age  +  prior  +  celltype  +  trt ,  \n                  data  =  veteran ,  shape  =   0 )  summary ( model2 )   ## Call:\n## weibreg(formula = y ~ karno + diagtime + age + prior + celltype + \n##     trt, data = veteran, shape = 0)\n## \n## Covariate           Mean       Coef Exp(Coef)  se(Coef)    Wald p\n## karno              68.419    -0.032     0.968     0.005     0.000 \n## diagtime            8.139     0.001     1.001     0.009     0.955 \n## age                57.379    -0.007     0.993     0.009     0.476 \n## prior \n##               no    0.653     0         1           (reference)\n##              yes    0.347     0.047     1.048     0.229     0.836 \n## celltype \n##            large    0.269     0         1           (reference)\n##         squamous    0.421    -0.428     0.651     0.278     0.123 \n##        smallcell    0.206     0.462     1.587     0.262     0.078 \n##            adeno    0.104     0.792     2.208     0.300     0.008 \n## trt \n##         standard    0.477     0         1           (reference)\n##             test    0.523     0.246     1.279     0.203     0.224 \n## \n## log(scale)                    2.864    17.537     0.671     0.000 \n## log(shape)                    0.075     1.077     0.066     0.261 \n## \n## Events                    128 \n## Total time at risk         16663 \n## Max. log. likelihood      -715.55 \n## LR test statistic         65.1 \n## Degrees of freedom        8 \n## Overall p-value           4.65393e-11  Finally, we can also fit a Cox proportional hazard:  model3  -  coxph ( y  ~  karno  +  diagtime  +  age  +  prior  +  celltype  +  trt ,  \n                data  =  veteran )  summary ( model3 )   ## Call:\n## coxph(formula = y ~ karno + diagtime + age + prior + celltype + \n##     trt, data = veteran)\n## \n##   n= 137, number of events= 128 \n## \n##                         coef  exp(coef)   se(coef)      z Pr( |z|)    \n## karno             -3.282e-02  9.677e-01  5.508e-03 -5.958 2.55e-09 ***\n## diagtime           8.132e-05  1.000e+00  9.136e-03  0.009  0.99290    \n## age               -8.706e-03  9.913e-01  9.300e-03 -0.936  0.34920    \n## prioryes           7.159e-02  1.074e+00  2.323e-01  0.308  0.75794    \n## celltypesquamous  -4.013e-01  6.695e-01  2.827e-01 -1.420  0.15574    \n## celltypesmallcell  4.603e-01  1.584e+00  2.662e-01  1.729  0.08383 .  \n## celltypeadeno      7.948e-01  2.214e+00  3.029e-01  2.624  0.00869 ** \n## trttest            2.946e-01  1.343e+00  2.075e-01  1.419  0.15577    \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n##                   exp(coef) exp(-coef) lower .95 upper .95\n## karno                0.9677     1.0334    0.9573    0.9782\n## diagtime             1.0001     0.9999    0.9823    1.0182\n## age                  0.9913     1.0087    0.9734    1.0096\n## prioryes             1.0742     0.9309    0.6813    1.6937\n## celltypesquamous     0.6695     1.4938    0.3847    1.1651\n## celltypesmallcell    1.5845     0.6311    0.9403    2.6699\n## celltypeadeno        2.2139     0.4517    1.2228    4.0084\n## trttest              1.3426     0.7448    0.8939    2.0166\n## \n## Concordance= 0.736  (se = 0.03 )\n## Rsquare= 0.364   (max possible= 0.999 )\n## Likelihood ratio test= 62.1  on 8 df,   p=1.799e-10\n## Wald test            = 62.37  on 8 df,   p=1.596e-10\n## Score (logrank) test = 66.74  on 8 df,   p=2.186e-11  As we can see, all three models are significant, and they give similar\ninformation:  karno  and  celltype  are significant predictors, both\ntreatment is not.  The method available in this package makes use of  case-base sampling .\nThat is, person-moments are randomly sampled across the entire follow-up\ntime, with some moments corresponding to cases and others to controls.\nBy sampling person-moments instead of individuals, we can then use\nlogistic regression to fit smooth-in-time parametric hazard functions.\nSee the previous section for more details.  First, we will look at the follow-up time by using population-time\nplots:  # create popTime object \npt_veteran  -  casebase :: popTime ( data  =  veteran )   ##  time  will be used as the time variable\n\n##  status  will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  class ( pt_veteran )   ## [1]  popTime      data.table   data.frame   # plot method for objects of class  popTime \nplot ( pt_veteran )    Population-time plots are a useful way of visualizing the total\nfollow-up experience, where individuals appear on the y-axis, and\nfollow-up time on the x-axis; each individual's follow-up time is\nrepresented by a gray line segment. For convenience, we have ordered the\npatients according to their time-to-event, and each event is represented\nby a red dot. The censored observations (of which there is only a few)\ncorrespond to the grey lines which do not end with a red dot.  Next, we use case-base sampling to fit a parametric hazard function via\nlogistic regression. First, we will include time as a linear term; as\nnoted above, this corresponds to an Gompertz hazard.  library ( casebase ) \nmodel4  -  fitSmoothHazard ( status  ~  time  +  karno  +  diagtime  +  age  +  prior  + \n             celltype  +  trt ,  data  =  veteran ,  ratio  =   100 ,  type  =   uniform )   ##  time  will be used as the time variable  summary ( model4 )   ## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.6321  -0.1471  -0.1326  -0.1180   3.3378  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr( |z|)    \n## (Intercept)       -4.723692   0.680388  -6.943 3.85e-12 ***\n## time               0.004259   0.000580   7.343 2.09e-13 ***\n## karno             -0.010888   0.004952  -2.199   0.0279 *  \n## diagtime           0.005922   0.009123   0.649   0.5163    \n## age                0.004306   0.009273   0.464   0.6424    \n## prioryes          -0.221476   0.230259  -0.962   0.3361    \n## celltypesquamous  -0.441758   0.287345  -1.537   0.1242    \n## celltypesmallcell  0.022139   0.259385   0.085   0.9320    \n## celltypeadeno      0.186398   0.291500   0.639   0.5225    \n## trttest           -0.100691   0.190795  -0.528   0.5977    \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1436.2  on 12927  degrees of freedom\n## Residual deviance: 1394.6  on 12918  degrees of freedom\n## AIC: 1414.6\n## \n## Number of Fisher Scoring iterations: 7  Since the output object from  fitSmoothHazard  inherits from the  glm \nclass, we see a familiar result when using the function  summary .  The main purpose of fitting smooth hazard functions is that it is then\nrelatively easy to compute absolute risks. For example, we can use the\nfunction  absoluteRisk  to compute the mean absolute risk at 90 days,\nwhich can then be compared to the empirical measure.  absoluteRisk ( object  =  model4 ,  time  =   90 )   ## [1] 0.4490265  ftime  -  veteran $ time mean ( ftime  =   90 )   ## [1] 0.5547445  We can also fit a Weibull hazard by using a logarithmic term for time:  model5  -  fitSmoothHazard ( status  ~   log ( time )   +  karno  +  diagtime  +  age  +  prior  + \n             celltype  +  trt ,  data  =  veteran ,  ratio  =   100 ,  type  =   uniform )   ##  time  will be used as the time variable  summary ( model5 )   ## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.3669  -0.1625  -0.1255  -0.0916   3.7345  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr( |z|)    \n## (Intercept)       -6.025256   0.715645  -8.419    2e-16 ***\n## log(time)          0.638999   0.081654   7.826 5.05e-15 ***\n## karno             -0.021859   0.005402  -4.046 5.21e-05 ***\n## diagtime           0.002051   0.008846   0.232    0.817    \n## age               -0.002489   0.009044  -0.275    0.783    \n## prioryes           0.038160   0.221385   0.172    0.863    \n## celltypesquamous  -0.160837   0.273353  -0.588    0.556    \n## celltypesmallcell  0.325276   0.263553   1.234    0.217    \n## celltypeadeno      0.475734   0.296983   1.602    0.109    \n## trttest            0.115654   0.191121   0.605    0.545    \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1436.2  on 12927  degrees of freedom\n## Residual deviance: 1363.0  on 12918  degrees of freedom\n## AIC: 1383\n## \n## Number of Fisher Scoring iterations: 8  With case-base sampling, it is straightforward to fit a semi-parametric\nhazard function using splines, which can then be used to estimate the\nmean absolute risk.  # Fit a spline for time  library ( splines ) \nmodel6  -  fitSmoothHazard ( status  ~  bs ( time )   +  karno  +  diagtime  +  age  +  prior  + \n             celltype  +  trt ,  data  =  veteran ,  ratio  =   100 ,  type  =   uniform )   ##  time  will be used as the time variable  summary ( model6 )   ## \n## Call:\n## glm(formula = formula, family = binomial, data = sampleData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.5950  -0.1519  -0.1207  -0.0984   3.4902  \n## \n## Coefficients:\n##                    Estimate Std. Error z value Pr( |z|)    \n## (Intercept)       -4.850575   0.688886  -7.041 1.91e-12 ***\n## bs(time)1          6.624422   1.057781   6.263 3.79e-10 ***\n## bs(time)2         -2.303337   1.859357  -1.239 0.215426    \n## bs(time)3          4.793540   0.954585   5.022 5.12e-07 ***\n## karno             -0.019221   0.005319  -3.614 0.000302 ***\n## diagtime           0.001291   0.009591   0.135 0.892906    \n## age               -0.002556   0.009120  -0.280 0.779292    \n## prioryes          -0.040716   0.229840  -0.177 0.859390    \n## celltypesquamous  -0.220063   0.281321  -0.782 0.434069    \n## celltypesmallcell  0.365648   0.268621   1.361 0.173449    \n## celltypeadeno      0.542413   0.303971   1.784 0.074355 .  \n## trttest            0.108617   0.197434   0.550 0.582222    \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1436.2  on 12927  degrees of freedom\n## Residual deviance: 1367.3  on 12916  degrees of freedom\n## AIC: 1391.3\n## \n## Number of Fisher Scoring iterations: 8  absoluteRisk ( object  =  model6 ,  time  =   90 )   ## [1] 0.4611582  As we can see from the summary, there is little evidence that splines\nactually improve the fit. Moreover, we can see that estimated individual\nabsolute risks are essentially the same when using either a linear term\nor splines:  linearRisk  -  absoluteRisk ( object  =  model4 ,  time  =   90 ,  newdata  =  veteran ) \nsplineRisk  -  absoluteRisk ( object  =  model6 ,  time  =   90 ,  newdata  =  veteran ) \n\nplot ( linearRisk ,  splineRisk , \n     xlab  =   Linear ,  ylab  =   Splines ,  pch  =   19 ) \nabline ( a  =   0 ,  b  =   1 ,  lty  =   2 ,  lwd  =   2 ,  col  =   red )    These last three models give similar information as the first three,\ni.e. the main predictors for the hazard are  karno  and  celltype , with\ntreatment being non-significant. Moreover, by explicitely including the\ntime variable in the formula, we see that it is not significant; this is\nevidence that the true hazard is exponential.  Finally, we can look at the estimates of the coefficients for the Cox\nmodel, as well as the last three models (CB stands for \"case-base\"):       \nCox model  \nCB linear  \nCB log-linear  \nCB splines      \nkarno  \n-0.0328  \n-0.0109  \n-0.0219  \n-0.0192    \ndiagtime  \n0.0001  \n0.0059  \n0.0021  \n0.0013    \nage  \n-0.0087  \n0.0043  \n-0.0025  \n-0.0026    \nprioryes  \n0.0716  \n-0.2215  \n0.0382  \n-0.0407    \ncelltypesquamous  \n-0.4013  \n-0.4418  \n-0.1608  \n-0.2201    \ncelltypesmallcell  \n0.4603  \n0.0221  \n0.3253  \n0.3656    \ncelltypeadeno  \n0.7948  \n0.1864  \n0.4757  \n0.5424    \ntrttest  \n0.2946  \n-0.1007  \n0.1157  \n0.1086", 
            "title": "First example"
        }, 
        {
            "location": "/smoothHazard/#cumulative-incidence-curves", 
            "text": "Here we show how to calculate the cumulative incidence curves for a\nspecific risk profile using the following equation:    CI(x, t) = 1 - exp\\left[ - \\int_0^t h(x, u) \\textrm{d}u \\right]  \nwhere \\( h(x, t) \\) is the hazard function, \\( t \\) denotes the\nnumerical value (number of units) of a point in prognostic/prospective\ntime and \\( x \\) is the realization of the vector \\( X \\) of\nvariates based on the patient's profile and intervention (if any).  We compare the cumulative incidence functions from the fully-parametric\nfit using case base sampling, with those from the Cox model:  # define a specific covariate profile \nnew_data  -   data.frame ( trt  =   test ,  \n                       celltype  =   adeno ,  \n                       karno  =  median ( veteran $ karno ),  \n                       diagtime  =  median ( veteran $ diagtime ), \n                       age  =  median ( veteran $ age ), \n                       prior  =   no )  # calculate cumulative incidence using casebase model \nsmooth_risk  -  absoluteRisk ( object  =  model4 ,  time  =   seq ( 0 , 300 ,   1 ),  \n                            newdata  =  new_data )  # cumulative incidence function for the Cox model \nplot ( survfit ( model3 ,  newdata = new_data ), \n     xlab  =   Days ,  ylab = Cumulative Incidence (%) ,  fun  =   event , \n     xlim  =   c ( 0 , 300 ),  conf.int  =   F ,  col  =   red ,  \n     main  =   sprintf ( Estimated Cumulative Incidence (risk) of Lung Cancer\\ntrt = test, celltype = adeno, karno = %g,\\ndiagtime = %g, age = %g, prior = no ,  median ( veteran $ karno ),  median ( veteran $ diagtime ),  \n                    median ( veteran $ age )))  # add casebase curve with legend \nlines ( seq ( 0 , 300 ,   1 ),  smooth_risk [ 1 ,],  type  =   l ,  col  =   blue ) \nlegend ( bottomright ,  \n       legend  =   c ( semi-parametric (Cox) ,   parametric (casebase) ),  \n       col  =   c ( red , blue ), \n       lty  =   c ( 1 ,   1 ),  \n       bg  =   gray90 )", 
            "title": "Cumulative Incidence Curves"
        }, 
        {
            "location": "/smoothHazard/#session-information", 
            "text": "## R version 3.3.1 (2016-06-21)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.10\n## \n## attached base packages:\n## [1] splines   stats     graphics  grDevices utils     datasets  methods  \n## [8] base     \n## \n## other attached packages:\n## [1] casebase_0.1.0  eha_2.4-4       survival_2.39-5\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.9      knitr_1.15.1     magrittr_1.5     munsell_0.4.3   \n##  [5] colorspace_1.3-1 lattice_0.20-33  highr_0.6        plyr_1.8.4      \n##  [9] stringr_1.2.0    tools_3.3.1      grid_3.3.1       data.table_1.9.6\n## [13] gtable_0.2.0     pacman_0.4.1     htmltools_0.3.5  assertthat_0.1  \n## [17] lazyeval_0.2.0   yaml_2.1.14      rprojroot_1.2    digest_0.6.12   \n## [21] tibble_1.2       Matrix_1.2-6     ggplot2_2.2.0    VGAM_1.0-2      \n## [25] evaluate_0.10    rmarkdown_1.3    labeling_0.3     stringi_1.1.2   \n## [29] scales_0.4.1     backports_1.0.5  stats4_3.3.1     chron_2.3-47", 
            "title": "Session information"
        }, 
        {
            "location": "/popTime/", 
            "text": "Load Required Packages\n\n\nlibrary\n(\nknitr\n)\n\n\nlibrary\n(\ndata.table\n)\n\n\nlibrary\n(\nmagrittr\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\nsurvival\n)\n\n\nlibrary\n(\ncasebase\n)\n\n\n\n\n\n\nVeteran Data\n\n\n# veteran data in library(survival)\n\ndata\n(\nveteran\n)\n\nstr\n(\nveteran\n)\n\n\n\n\n\n\n## \ndata.frame\n:    137 obs. of  8 variables:\n##  $ trt     : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ celltype: Factor w/ 4 levels \nsquamous\n,\nsmallcell\n,..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time    : num  72 411 228 126 118 10 82 110 314 100 ...\n##  $ status  : num  1 1 1 1 1 1 1 1 1 0 ...\n##  $ karno   : num  60 70 60 60 70 20 40 80 50 70 ...\n##  $ diagtime: num  7 5 3 9 11 5 10 29 18 6 ...\n##  $ age     : num  69 64 38 63 65 49 69 68 43 70 ...\n##  $ prior   : num  0 10 0 10 10 0 10 0 0 0 ...\n\n\n\n\n\n# create \npopTime\n object\n\npopTimeData \n-\n popTime\n(\ndata \n=\n veteran\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n## \nstatus\n will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\n# object of class \npopTime\n\n\nclass\n(\npopTimeData\n)\n\n\n\n\n\n\n## [1] \npopTime\n    \ndata.table\n \ndata.frame\n\n\n\n\n\n\n# plot method for objects of class \npopTime\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\nStratified by treatment population time plot\n\n\n# stratified by treatment population time plot\n\nveteran \n-\n \ntransform\n(\nveteran\n,\n trt \n=\n \nfactor\n(\ntrt\n,\n levels \n=\n \n1\n:\n2\n,\n\n                                           labels \n=\n \nc\n(\nstandard\n,\n \ntest\n)))\n\n\n\n# create \npopTimeExposure\n object\n\npopTimeData \n-\n popTime\n(\ndata \n=\n veteran\n,\n exposure \n=\n \ntrt\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n## \nstatus\n will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\n# object of class \npopTimeExposure\n\n\nclass\n(\npopTimeData\n)\n\n\n\n\n\n\n## [1] \npopTimeExposure\n \nlist\n\n\n\n\n\n\n# plot method for objects of class \npopTimeExposure\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\nStem Cell Data\n\n\ndata\n(\nbmtcrr\n)\n\nstr\n(\nbmtcrr\n)\n\n\n\n\n\n\n## \ndata.frame\n:    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels \nF\n,\nM\n: 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \nALL\n,\nAML\n: 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels \nCR1\n,\nCR2\n,\nCR3\n,..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \nBM+PB\n,\nPB\n: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n\n\n\n\n\n# create \npopTime\n object\n\npopTimeData \n-\n popTime\n(\ndata \n=\n bmtcrr\n,\n time \n=\n \nftime\n)\n\n\n\n\n\n\n## \nStatus\n will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\n# object of class \npopTime\n\n\nclass\n(\npopTimeData\n)\n\n\n\n\n\n\n## [1] \npopTime\n    \ndata.table\n \ndata.frame\n\n\n\n\n\n\n# plot method for objects of class \npopTime\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\nStratified by Disease\n\n\n# stratified by Disease population time plot\n\n\n# Disease (lymphoblastic or myeloblastic leukemia,\n\n\n# abbreviated as ALL and AML, respectively)\n\n\n\n# create \npopTimeExposure\n object\n\npopTimeData \n-\n popTime\n(\ndata \n=\n bmtcrr\n,\n time \n=\n \nftime\n,\n exposure \n=\n \nD\n)\n\n\n\n\n\n\n## \nStatus\n will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\n# object of class \npopTimeExposure\n\n\nclass\n(\npopTimeData\n)\n\n\n\n\n\n\n## [1] \npopTimeExposure\n \nlist\n\n\n\n\n\n\n# plot method for objects of class \npopTimeExposure\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\n# stratify by gender\n\npopTimeData \n-\n popTime\n(\ndata \n=\n bmtcrr\n,\n time \n=\n \nftime\n,\n exposure \n=\n \nSex\n)\n\n\n\n\n\n\n## \nStatus\n will be used as the event variable\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\nStanford Heart Transplant Data\n\n\n# data from library(survival)\n\ndata\n(\nheart\n)\n\nstr\n(\nheart\n)\n\n\n\n\n\n\n## \ndata.frame\n:    172 obs. of  8 variables:\n##  $ start     : num  0 0 0 1 0 36 0 0 0 51 ...\n##  $ stop      : num  50 6 1 16 36 39 18 3 51 675 ...\n##  $ event     : num  1 1 0 1 0 1 1 1 0 1 ...\n##  $ age       : num  -17.16 3.84 6.3 6.3 -7.74 ...\n##  $ year      : num  0.123 0.255 0.266 0.266 0.49 ...\n##  $ surgery   : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ transplant: Factor w/ 2 levels \n0\n,\n1\n: 1 1 1 2 1 2 1 1 1 2 ...\n##  $ id        : num  1 2 3 3 4 4 5 6 7 7 ...\n\n\n\n\n\n# create time variable for time in study\n\nheart \n-\n \ntransform\n(\nheart\n,\n\n                   time \n=\n stop \n-\n start\n,\n\n                   transplant \n=\n \nfactor\n(\ntransplant\n,\n\n                                       labels \n=\n \nc\n(\nno transplant\n,\n \ntransplant\n)))\n\n\n\n# stratify by transplant indicator\n\npopTimeData \n-\n popTime\n(\ndata \n=\n heart\n,\n exposure \n=\n \ntransplant\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n## \nevent\n will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n## Sampling only from individuals who never experienced\n##                     the event of interest\n\n\n\n\n\n# can specify a legend\n\nplot\n(\npopTimeData\n,\n legend \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\nNCCTG Lung Cancer Data\n\n\n# data from library(survival)\n\ndata\n(\ncancer\n)\n\nstr\n(\ncancer\n)\n\n\n\n\n\n\n## \ndata.frame\n:    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n\n\n\n\n# since the event indicator \nstatus\n is numeric, it must have\n\n\n# 0 for censored and 1 for event\n\ncancer \n-\n \ntransform\n(\ncancer\n,\n\n                    status \n=\n status \n-\n \n1\n,\n\n                    sex \n=\n \nfactor\n(\nsex\n,\n levels \n=\n \n1\n:\n2\n,\n\n                                 labels \n=\n \nc\n(\nMale\n,\n \nFemale\n)))\n\n\n\n\n# population time plot\n\n\n# redistributing the red points among those who never experienced an event\n\n\n# because there are enough available at each time point\n\npopTimeData \n-\n popTime\n(\ndata \n=\n cancer\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n## \nstatus\n will be used as the event variable\n\n## Sampling only from individuals who never experienced\n##                     the event of interest\n\n\n\n\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\nStratified by gender\n\n\npopTimeData \n-\n popTime\n(\ndata \n=\n cancer\n,\n exposure \n=\n \nsex\n)\n\n\n\n\n\n\n## \ntime\n will be used as the time variable\n\n## \nstatus\n will be used as the event variable\n\n## Sampling only from individuals who never experienced\n##                     the event of interest\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\n# can change the plot aesthetics\n\nplot\n(\npopTimeData\n,\n\n     line.width \n=\n \n0.2\n,\n line.colour \n=\n \nblack\n,\n\n     point.size \n=\n \n1\n,\n point.colour \n=\n \ncyan\n)\n\n\n\n\n\n\n\n\nSimulated Data Example\n\n\nSimulate the data\n\n\nset.seed\n(\n1\n)\n\nnobs \n-\n \n5000\n\n\n\n# simulation parameters\n\na1 \n-\n \n1.0\n\nb1 \n-\n \n200\n\na2 \n-\n \n1.0\n\nb2 \n-\n \n50\n\nc1 \n-\n \n0.0\n\nc2 \n-\n \n0.0\n\n\n\n# end of study time\n\neost \n-\n \n10\n\n\n\n# e event type 0-censored, 1-event of interest, 2-competing event\n\n\n# t observed time/endpoint\n\n\n# z is a binary covariate\n\nDTsim \n-\n data.table\n(\nID \n=\n \nseq_len\n(\nnobs\n),\n z\n=\nrbinom\n(\nnobs\n,\n \n1\n,\n \n0.5\n))\n\nsetkey\n(\nDTsim\n,\n ID\n)\n\nDTsim\n[,\n`:=`\n \n(\nevent_time \n=\n rweibull\n(\nnobs\n,\n a1\n,\n b1 \n*\n \nexp\n(\nz \n*\n c1\n)\n^\n(\n-1\n/\na1\n)),\n\n             competing_time \n=\n rweibull\n(\nnobs\n,\n a2\n,\n b2 \n*\n \nexp\n(\nz \n*\n c2\n)\n^\n(\n-1\n/\na2\n)),\n\n             end_of_study_time \n=\n eost\n)]\n\n\n\n\n\n\n##         ID z event_time competing_time end_of_study_time\n##    1:    1 0  667.64719     136.890050                10\n##    2:    2 0  206.73147      19.532835                10\n##    3:    3 1  278.16256      15.368960                10\n##    4:    4 1   25.75304     109.776451                10\n##    5:    5 0  229.15406     153.249168                10\n##   ---                                                   \n## 4996: 4996 0   62.37659     136.007260                10\n## 4997: 4997 0  139.18781      21.246797                10\n## 4998: 4998 0  137.28346       3.674155                10\n## 4999: 4999 0  113.08585       7.509247                10\n## 5000: 5000 0   85.33457      11.294627                10\n\n\n\n\n\nDTsim\n[,\n`:=`\n(\nevent \n=\n \n1\n \n*\n \n(\nevent_time \n competing_time\n)\n \n+\n\n                \n2\n \n*\n \n(\nevent_time \n=\n competing_time\n),\n\n            time \n=\n \npmin\n(\nevent_time\n,\n competing_time\n))]\n\n\n\n\n\n\n##         ID z event_time competing_time end_of_study_time event       time\n##    1:    1 0  667.64719     136.890050                10     2 136.890050\n##    2:    2 0  206.73147      19.532835                10     2  19.532835\n##    3:    3 1  278.16256      15.368960                10     2  15.368960\n##    4:    4 1   25.75304     109.776451                10     1  25.753040\n##    5:    5 0  229.15406     153.249168                10     2 153.249168\n##   ---                                                                    \n## 4996: 4996 0   62.37659     136.007260                10     1  62.376591\n## 4997: 4997 0  139.18781      21.246797                10     2  21.246797\n## 4998: 4998 0  137.28346       3.674155                10     2   3.674155\n## 4999: 4999 0  113.08585       7.509247                10     2   7.509247\n## 5000: 5000 0   85.33457      11.294627                10     2  11.294627\n\n\n\n\n\nDTsim\n[\ntime \n=\n end_of_study_time\n,\n event \n:=\n \n0\n]\n\n\n\n\n\n\n##         ID z event_time competing_time end_of_study_time event       time\n##    1:    1 0  667.64719     136.890050                10     0 136.890050\n##    2:    2 0  206.73147      19.532835                10     0  19.532835\n##    3:    3 1  278.16256      15.368960                10     0  15.368960\n##    4:    4 1   25.75304     109.776451                10     0  25.753040\n##    5:    5 0  229.15406     153.249168                10     0 153.249168\n##   ---                                                                    \n## 4996: 4996 0   62.37659     136.007260                10     0  62.376591\n## 4997: 4997 0  139.18781      21.246797                10     0  21.246797\n## 4998: 4998 0  137.28346       3.674155                10     2   3.674155\n## 4999: 4999 0  113.08585       7.509247                10     2   7.509247\n## 5000: 5000 0   85.33457      11.294627                10     0  11.294627\n\n\n\n\n\nDTsim\n[\ntime \n=\n end_of_study_time\n,\n time\n:=\nend_of_study_time\n]\n\n\n\n\n\n\n##         ID z event_time competing_time end_of_study_time event      time\n##    1:    1 0  667.64719     136.890050                10     0 10.000000\n##    2:    2 0  206.73147      19.532835                10     0 10.000000\n##    3:    3 1  278.16256      15.368960                10     0 10.000000\n##    4:    4 1   25.75304     109.776451                10     0 10.000000\n##    5:    5 0  229.15406     153.249168                10     0 10.000000\n##   ---                                                                   \n## 4996: 4996 0   62.37659     136.007260                10     0 10.000000\n## 4997: 4997 0  139.18781      21.246797                10     0 10.000000\n## 4998: 4998 0  137.28346       3.674155                10     2  3.674155\n## 4999: 4999 0  113.08585       7.509247                10     2  7.509247\n## 5000: 5000 0   85.33457      11.294627                10     0 10.000000\n\n\n\n\n\nPopulation Time Plot\n\n\n# create \npopTime\n object\n\npopTimeData \n-\n popTime\n(\ndata \n=\n DTsim\n,\n time \n=\n \ntime\n,\n event \n=\n \nevent\n)\n\n\n\n\n\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\nplot\n(\npopTimeData\n)\n\n\n\n\n\n\n\n\nStratified by Binary Covariate z\n\n\n# stratified by binary covariate z\n\npopTimeData \n-\n popTime\n(\ndata \n=\n DTsim\n,\n time \n=\n \ntime\n,\n event \n=\n \nevent\n,\n exposure \n=\n \nz\n)\n\n\n\n\n\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n\n\n\n\n# we can line up the plots side-by-side instead of one on top of the other\n\nplot\n(\npopTimeData\n,\n ncol \n=\n \n2\n)\n\n\n\n\n\n\n\n\nSession information\n\n\n## R version 3.3.1 (2016-06-21)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.10\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n## [1] casebase_0.1.0   survival_2.39-5  ggplot2_2.2.0    magrittr_1.5    \n## [5] data.table_1.9.6 knitr_1.15.1    \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.9      splines_3.3.1    munsell_0.4.3    lattice_0.20-33 \n##  [5] colorspace_1.3-1 stringr_1.2.0    plyr_1.8.4       tools_3.3.1     \n##  [9] grid_3.3.1       gtable_0.2.0     htmltools_0.3.5  yaml_2.1.14     \n## [13] lazyeval_0.2.0   rprojroot_1.2    digest_0.6.12    assertthat_0.1  \n## [17] tibble_1.2       Matrix_1.2-6     VGAM_1.0-2       evaluate_0.10   \n## [21] rmarkdown_1.3    labeling_0.3     stringi_1.1.2    scales_0.4.1    \n## [25] backports_1.0.5  stats4_3.3.1     chron_2.3-47", 
            "title": "Population Time Plots"
        }, 
        {
            "location": "/popTime/#load-required-packages", 
            "text": "library ( knitr )  library ( data.table )  library ( magrittr )  library ( ggplot2 )  library ( survival )  library ( casebase )", 
            "title": "Load Required Packages"
        }, 
        {
            "location": "/popTime/#veteran-data", 
            "text": "# veteran data in library(survival) \ndata ( veteran ) \nstr ( veteran )   ##  data.frame :    137 obs. of  8 variables:\n##  $ trt     : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ celltype: Factor w/ 4 levels  squamous , smallcell ,..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time    : num  72 411 228 126 118 10 82 110 314 100 ...\n##  $ status  : num  1 1 1 1 1 1 1 1 1 0 ...\n##  $ karno   : num  60 70 60 60 70 20 40 80 50 70 ...\n##  $ diagtime: num  7 5 3 9 11 5 10 29 18 6 ...\n##  $ age     : num  69 64 38 63 65 49 69 68 43 70 ...\n##  $ prior   : num  0 10 0 10 10 0 10 0 0 0 ...  # create  popTime  object \npopTimeData  -  popTime ( data  =  veteran )   ##  time  will be used as the time variable\n\n##  status  will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  # object of class  popTime  class ( popTimeData )   ## [1]  popTime      data.table   data.frame   # plot method for objects of class  popTime \nplot ( popTimeData )", 
            "title": "Veteran Data"
        }, 
        {
            "location": "/popTime/#stratified-by-treatment-population-time-plot", 
            "text": "# stratified by treatment population time plot \nveteran  -   transform ( veteran ,  trt  =   factor ( trt ,  levels  =   1 : 2 , \n                                           labels  =   c ( standard ,   test )))  # create  popTimeExposure  object \npopTimeData  -  popTime ( data  =  veteran ,  exposure  =   trt )   ##  time  will be used as the time variable\n\n##  status  will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  # object of class  popTimeExposure  class ( popTimeData )   ## [1]  popTimeExposure   list   # plot method for objects of class  popTimeExposure \nplot ( popTimeData )", 
            "title": "Stratified by treatment population time plot"
        }, 
        {
            "location": "/popTime/#stem-cell-data", 
            "text": "data ( bmtcrr ) \nstr ( bmtcrr )   ##  data.frame :    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels  F , M : 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels  ALL , AML : 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels  CR1 , CR2 , CR3 ,..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels  BM+PB , PB : 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...  # create  popTime  object \npopTimeData  -  popTime ( data  =  bmtcrr ,  time  =   ftime )   ##  Status  will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  # object of class  popTime  class ( popTimeData )   ## [1]  popTime      data.table   data.frame   # plot method for objects of class  popTime \nplot ( popTimeData )", 
            "title": "Stem Cell Data"
        }, 
        {
            "location": "/popTime/#stratified-by-disease", 
            "text": "# stratified by Disease population time plot  # Disease (lymphoblastic or myeloblastic leukemia,  # abbreviated as ALL and AML, respectively)  # create  popTimeExposure  object \npopTimeData  -  popTime ( data  =  bmtcrr ,  time  =   ftime ,  exposure  =   D )   ##  Status  will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  # object of class  popTimeExposure  class ( popTimeData )   ## [1]  popTimeExposure   list   # plot method for objects of class  popTimeExposure \nplot ( popTimeData )    # stratify by gender \npopTimeData  -  popTime ( data  =  bmtcrr ,  time  =   ftime ,  exposure  =   Sex )   ##  Status  will be used as the event variable\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  plot ( popTimeData )", 
            "title": "Stratified by Disease"
        }, 
        {
            "location": "/popTime/#stanford-heart-transplant-data", 
            "text": "# data from library(survival) \ndata ( heart ) \nstr ( heart )   ##  data.frame :    172 obs. of  8 variables:\n##  $ start     : num  0 0 0 1 0 36 0 0 0 51 ...\n##  $ stop      : num  50 6 1 16 36 39 18 3 51 675 ...\n##  $ event     : num  1 1 0 1 0 1 1 1 0 1 ...\n##  $ age       : num  -17.16 3.84 6.3 6.3 -7.74 ...\n##  $ year      : num  0.123 0.255 0.266 0.266 0.49 ...\n##  $ surgery   : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ transplant: Factor w/ 2 levels  0 , 1 : 1 1 1 2 1 2 1 1 1 2 ...\n##  $ id        : num  1 2 3 3 4 4 5 6 7 7 ...  # create time variable for time in study \nheart  -   transform ( heart , \n                   time  =  stop  -  start , \n                   transplant  =   factor ( transplant , \n                                       labels  =   c ( no transplant ,   transplant )))  # stratify by transplant indicator \npopTimeData  -  popTime ( data  =  heart ,  exposure  =   transplant )   ##  time  will be used as the time variable\n\n##  event  will be used as the event variable\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status\n\n## Sampling only from individuals who never experienced\n##                     the event of interest  # can specify a legend \nplot ( popTimeData ,  legend  =   TRUE )", 
            "title": "Stanford Heart Transplant Data"
        }, 
        {
            "location": "/popTime/#ncctg-lung-cancer-data", 
            "text": "# data from library(survival) \ndata ( cancer ) \nstr ( cancer )   ##  data.frame :    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...  # since the event indicator  status  is numeric, it must have  # 0 for censored and 1 for event \ncancer  -   transform ( cancer , \n                    status  =  status  -   1 , \n                    sex  =   factor ( sex ,  levels  =   1 : 2 , \n                                 labels  =   c ( Male ,   Female )))  # population time plot  # redistributing the red points among those who never experienced an event  # because there are enough available at each time point \npopTimeData  -  popTime ( data  =  cancer )   ##  time  will be used as the time variable\n\n##  status  will be used as the event variable\n\n## Sampling only from individuals who never experienced\n##                     the event of interest  plot ( popTimeData )", 
            "title": "NCCTG Lung Cancer Data"
        }, 
        {
            "location": "/popTime/#stratified-by-gender", 
            "text": "popTimeData  -  popTime ( data  =  cancer ,  exposure  =   sex )   ##  time  will be used as the time variable\n\n##  status  will be used as the event variable\n\n## Sampling only from individuals who never experienced\n##                     the event of interest\n\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  # can change the plot aesthetics \nplot ( popTimeData , \n     line.width  =   0.2 ,  line.colour  =   black , \n     point.size  =   1 ,  point.colour  =   cyan )", 
            "title": "Stratified by gender"
        }, 
        {
            "location": "/popTime/#simulated-data-example", 
            "text": "", 
            "title": "Simulated Data Example"
        }, 
        {
            "location": "/popTime/#simulate-the-data", 
            "text": "set.seed ( 1 ) \nnobs  -   5000  # simulation parameters \na1  -   1.0 \nb1  -   200 \na2  -   1.0 \nb2  -   50 \nc1  -   0.0 \nc2  -   0.0  # end of study time \neost  -   10  # e event type 0-censored, 1-event of interest, 2-competing event  # t observed time/endpoint  # z is a binary covariate \nDTsim  -  data.table ( ID  =   seq_len ( nobs ),  z = rbinom ( nobs ,   1 ,   0.5 )) \nsetkey ( DTsim ,  ID ) \nDTsim [, `:=`   ( event_time  =  rweibull ( nobs ,  a1 ,  b1  *   exp ( z  *  c1 ) ^ ( -1 / a1 )), \n             competing_time  =  rweibull ( nobs ,  a2 ,  b2  *   exp ( z  *  c2 ) ^ ( -1 / a2 )), \n             end_of_study_time  =  eost )]   ##         ID z event_time competing_time end_of_study_time\n##    1:    1 0  667.64719     136.890050                10\n##    2:    2 0  206.73147      19.532835                10\n##    3:    3 1  278.16256      15.368960                10\n##    4:    4 1   25.75304     109.776451                10\n##    5:    5 0  229.15406     153.249168                10\n##   ---                                                   \n## 4996: 4996 0   62.37659     136.007260                10\n## 4997: 4997 0  139.18781      21.246797                10\n## 4998: 4998 0  137.28346       3.674155                10\n## 4999: 4999 0  113.08585       7.509247                10\n## 5000: 5000 0   85.33457      11.294627                10  DTsim [, `:=` ( event  =   1   *   ( event_time   competing_time )   + \n                 2   *   ( event_time  =  competing_time ), \n            time  =   pmin ( event_time ,  competing_time ))]   ##         ID z event_time competing_time end_of_study_time event       time\n##    1:    1 0  667.64719     136.890050                10     2 136.890050\n##    2:    2 0  206.73147      19.532835                10     2  19.532835\n##    3:    3 1  278.16256      15.368960                10     2  15.368960\n##    4:    4 1   25.75304     109.776451                10     1  25.753040\n##    5:    5 0  229.15406     153.249168                10     2 153.249168\n##   ---                                                                    \n## 4996: 4996 0   62.37659     136.007260                10     1  62.376591\n## 4997: 4997 0  139.18781      21.246797                10     2  21.246797\n## 4998: 4998 0  137.28346       3.674155                10     2   3.674155\n## 4999: 4999 0  113.08585       7.509247                10     2   7.509247\n## 5000: 5000 0   85.33457      11.294627                10     2  11.294627  DTsim [ time  =  end_of_study_time ,  event  :=   0 ]   ##         ID z event_time competing_time end_of_study_time event       time\n##    1:    1 0  667.64719     136.890050                10     0 136.890050\n##    2:    2 0  206.73147      19.532835                10     0  19.532835\n##    3:    3 1  278.16256      15.368960                10     0  15.368960\n##    4:    4 1   25.75304     109.776451                10     0  25.753040\n##    5:    5 0  229.15406     153.249168                10     0 153.249168\n##   ---                                                                    \n## 4996: 4996 0   62.37659     136.007260                10     0  62.376591\n## 4997: 4997 0  139.18781      21.246797                10     0  21.246797\n## 4998: 4998 0  137.28346       3.674155                10     2   3.674155\n## 4999: 4999 0  113.08585       7.509247                10     2   7.509247\n## 5000: 5000 0   85.33457      11.294627                10     0  11.294627  DTsim [ time  =  end_of_study_time ,  time := end_of_study_time ]   ##         ID z event_time competing_time end_of_study_time event      time\n##    1:    1 0  667.64719     136.890050                10     0 10.000000\n##    2:    2 0  206.73147      19.532835                10     0 10.000000\n##    3:    3 1  278.16256      15.368960                10     0 10.000000\n##    4:    4 1   25.75304     109.776451                10     0 10.000000\n##    5:    5 0  229.15406     153.249168                10     0 10.000000\n##   ---                                                                   \n## 4996: 4996 0   62.37659     136.007260                10     0 10.000000\n## 4997: 4997 0  139.18781      21.246797                10     0 10.000000\n## 4998: 4998 0  137.28346       3.674155                10     2  3.674155\n## 4999: 4999 0  113.08585       7.509247                10     2  7.509247\n## 5000: 5000 0   85.33457      11.294627                10     0 10.000000", 
            "title": "Simulate the data"
        }, 
        {
            "location": "/popTime/#population-time-plot", 
            "text": "# create  popTime  object \npopTimeData  -  popTime ( data  =  DTsim ,  time  =   time ,  event  =   event )   ## Sampling from all remaining individuals under study,\n##                     regardless of event status  plot ( popTimeData )", 
            "title": "Population Time Plot"
        }, 
        {
            "location": "/popTime/#stratified-by-binary-covariate-z", 
            "text": "# stratified by binary covariate z \npopTimeData  -  popTime ( data  =  DTsim ,  time  =   time ,  event  =   event ,  exposure  =   z )   ## Sampling from all remaining individuals under study,\n##                     regardless of event status\n## Sampling from all remaining individuals under study,\n##                     regardless of event status  # we can line up the plots side-by-side instead of one on top of the other \nplot ( popTimeData ,  ncol  =   2 )", 
            "title": "Stratified by Binary Covariate z"
        }, 
        {
            "location": "/popTime/#session-information", 
            "text": "## R version 3.3.1 (2016-06-21)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.10\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n## [1] casebase_0.1.0   survival_2.39-5  ggplot2_2.2.0    magrittr_1.5    \n## [5] data.table_1.9.6 knitr_1.15.1    \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.9      splines_3.3.1    munsell_0.4.3    lattice_0.20-33 \n##  [5] colorspace_1.3-1 stringr_1.2.0    plyr_1.8.4       tools_3.3.1     \n##  [9] grid_3.3.1       gtable_0.2.0     htmltools_0.3.5  yaml_2.1.14     \n## [13] lazyeval_0.2.0   rprojroot_1.2    digest_0.6.12    assertthat_0.1  \n## [17] tibble_1.2       Matrix_1.2-6     VGAM_1.0-2       evaluate_0.10   \n## [21] rmarkdown_1.3    labeling_0.3     stringi_1.1.2    scales_0.4.1    \n## [25] backports_1.0.5  stats4_3.3.1     chron_2.3-47", 
            "title": "Session information"
        }, 
        {
            "location": "/competingRisk/", 
            "text": "Data\n\n\nWe will use the same data that was used in Scrucca \net al\n \n-@scrucca2010regression\n. The data is available on the main author's \nwebsite\n.\n\n\nset.seed\n(\n12345\n)\n\n\nlibrary\n(\ncasebase\n)\n\ndata\n(\nbmtcrr\n)\n\n\nhead\n(\nbmtcrr\n)\n\n\n\n\n\n\n##   Sex   D   Phase Age Status Source  ftime\n## 1   M ALL Relapse  48      2  BM+PB   0.67\n## 2   F AML     CR2  23      1  BM+PB   9.50\n## 3   M ALL     CR3   7      0  BM+PB 131.77\n## 4   F ALL     CR2  26      2  BM+PB  24.03\n## 5   F ALL     CR2  36      2  BM+PB   1.47\n## 6   M ALL Relapse  17      2  BM+PB   2.23\n\n\n\n\n\nWe will perform a competing risk analysis on data from 177 patients who received a stem cell transplant for acute leukemia. The event of interest in relapse, but other competing causes (e.g. transplant-related death) need to be taken into account. We also want to take into account the effect of several covariates such as Sex, Disease (lymphoblastic or myeloblastic leukemia, abbreviated as ALL and AML, respectively), Phase at transplant (Relapse, CR1, CR2, CR3), Source of stem cells (bone marrow and peripheral blood, coded as BM+PB, or peripheral blood, coded as PB), and Age. Below, we reproduce their Table 1:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\nStatistical summary\n\n\n\n\n\n\n\n\n\n\nSex\n\n\nSex\n\n\nM=Male (100) \n F=Female (77)\n\n\n\n\n\n\nD\n\n\nDisease\n\n\nALL (73) \n AML (104)\n\n\n\n\n\n\nPhase\n\n\nPhase\n\n\nCR1 (47) \n CR2 (45) \n CR3 (12) \n Relapse (73)\n\n\n\n\n\n\nSource\n\n\nType of transplant\n\n\nBM+PB (21) \n PB (156)\n\n\n\n\n\n\nAge\n\n\nAge of patient (years)\n\n\n4\u201362 \n 30.47 (13.04)\n\n\n\n\n\n\nFtime\n\n\nFailure time (months)\n\n\n0.13\u2013131.77 \n 20.28 (30.78)\n\n\n\n\n\n\nStatus\n\n\nStatus indicator\n\n\n0=censored (46) \n 1=relapse (56) \n 2=competing event (75)\n\n\n\n\n\n\n\n\n\nThe statistical summary is generated differently for continuous and categorical variables:\n\n\n\n\n\n\nFor continuous variables, we are given the range, followed by the mean and standard deviation.\n\n\n\n\n\n\nFor categorical variables, we are given the counts for each category.\n\n\n\n\n\n\nNote that failure time can also correspond to censoring.\n\n\nPopulation-time plots\n\n\nIn order to try and visualize the incidence density of relapse, we can look at a population-time plot: on the X-axis we have time, and on the Y-axis we have the size of the risk set at a particular time point. Failure times associated to the event of interest can then be highlighted on the plot using red dots.\n\n\nnobs \n-\n \nnrow\n(\nbmtcrr\n)\n\nftime \n-\n bmtcrr\n$\nftime\nord \n-\n \norder\n(\nftime\n,\n decreasing \n=\n \nFALSE\n)\n\n\n\n# We split the person-moments in four categories:\n\n\n# 1) at-risk\n\n\n# 2) main event\n\n\n# 3) competing event\n\n\n# 4) censored\n\nyCoords \n-\n \ncbind\n(\ncumsum\n(\nbmtcrr\n[\nord\n,\n \nStatus\n]\n \n==\n \n2\n),\n \n                 \ncumsum\n(\nbmtcrr\n[\nord\n,\n \nStatus\n]\n \n==\n \n1\n),\n\n                 \ncumsum\n(\nbmtcrr\n[\nord\n,\n \nStatus\n]\n \n==\n \n0\n))\n\nyCoords \n-\n \ncbind\n(\nyCoords\n,\n nobs \n-\n \nrowSums\n(\nyCoords\n))\n\n\n\n# Plot only at-risk\n\nplot\n(\n0\n,\n type \n=\n \nn\n,\n xlim \n=\n \nc\n(\n0\n,\n \nmax\n(\nftime\n)),\n ylim \n=\n \nc\n(\n0\n,\n nobs\n),\n \n     xlab \n=\n \nFollow-up time\n,\n ylab \n=\n \nPopulation\n)\n\npolygon\n(\nc\n(\n0\n,\n \n0\n,\n ftime\n[\nord\n],\n \nmax\n(\nftime\n),\n \n0\n),\n\n        \nc\n(\n0\n,\n nobs\n,\n yCoords\n[,\n4\n],\n \n0\n,\n \n0\n),\n col \n=\n \ngrey90\n)\n\ncases \n-\n bmtcrr\n[,\n \nStatus\n]\n \n==\n \n1\n\n\n\n# randomly move the cases vertically\n\nmoved_cases \n-\n yCoords\n[\ncases\n[\nord\n],\n \n4\n]\n \n*\n runif\n(\nsum\n(\ncases\n))\n\npoints\n((\nftime\n[\nord\n])[\ncases\n[\nord\n]],\n moved_cases\n,\n pch \n=\n \n20\n,\n \n       col \n=\n \nred\n,\n cex \n=\n \n1\n)\n\n\n\n\n\n\n\n\nWe can right away draw a few conclusions from this plot: first of all, we get a sense of how quickly the size of the risk set changes over time. We also see that the incidence density is non-constant: most relapses occur before 15 months. Finally, we also see that the risk set keeps shrinking after the last event has occured; this could be due to either censoring or the competing event.\n\n\nTo get an idea of whether only relapse is responsible for the shrinking of the risk set in the first few months of follow-up, we can also keep track of how many events have occured at each time point:\n\n\n# Plot at-risk and events\n\nplot\n(\n0\n,\n type \n=\n \nn\n,\n xlim \n=\n \nc\n(\n0\n,\n \nmax\n(\nftime\n)),\n ylim \n=\n \nc\n(\n0\n,\n nobs\n),\n \n     xlab \n=\n \nFollow-up time\n,\n ylab \n=\n \nPopulation\n)\n\npolygon\n(\nx \n=\n \nc\n(\n0\n,\nftime\n[\nord\n],\n \nmax\n(\nftime\n),\n \n0\n),\n \n        y \n=\n \nc\n(\n0\n,\n yCoords\n[,\n2\n],\n \n0\n,\n \n0\n),\n \n        col \n=\n \nfirebrick3\n)\n\npolygon\n(\nx \n=\n \nc\n(\n0\n,\n ftime\n[\nord\n],\n ftime\n[\nrev\n(\nord\n)],\n \n0\n,\n \n0\n),\n\n        y \n=\n \nc\n(\n0\n,\n yCoords\n[,\n2\n],\n \nrev\n(\nyCoords\n[,\n2\n]\n \n+\n yCoords\n[,\n4\n]),\n nobs\n,\n \n0\n),\n \n        col \n=\n \ngrey90\n)\n\n\n\n# randomly move the cases vertically\n\nmoved_cases \n-\n yCoords\n[\ncases\n[\nord\n],\n \n2\n]\n \n+\n yCoords\n[\ncases\n[\nord\n],\n \n4\n]\n \n*\n runif\n(\nsum\n(\ncases\n))\n\npoints\n((\nftime\n[\nord\n])[\ncases\n[\nord\n]],\n moved_cases\n,\n pch \n=\n \n20\n,\n\n       col \n=\n \nred\n,\n cex \n=\n \n1\n)\n\nlegend\n(\ntopright\n,\n legend \n=\n \nc\n(\nRelapse\n,\n \nAt-risk\n),\n \n       col \n=\n \nc\n(\nfirebrick3\n,\n \ngrey90\n),\n\n       pch \n=\n \n15\n)\n\n\n\n\n\n\n\n\nTherefore, there is also censoring and loss due to competing events happening in the first few months. However, with this plot, we can't differentiate bwetween the two contributions. For this reason we can also keep track of the number of competing events at each time point:\n\n\nplot\n(\n0\n,\n type \n=\n \nn\n,\n xlim \n=\n \nc\n(\n0\n,\n \nmax\n(\nftime\n)),\n ylim \n=\n \nc\n(\n0\n,\n nobs\n),\n \n     xlab \n=\n \nFollow-up time\n,\n ylab \n=\n \nPopulation\n)\n\npolygon\n(\nx \n=\n \nc\n(\n0\n,\n \nmax\n(\nftime\n),\n \nmax\n(\nftime\n),\n \n0\n),\n\n        y \n=\n \nc\n(\n0\n,\n \n0\n,\n nobs\n,\n nobs\n),\n col \n=\n \nwhite\n)\n\n\n# Event of interest\n\npolygon\n(\nx \n=\n \nc\n(\n0\n,\nftime\n[\nord\n],\n \nmax\n(\nftime\n),\n \n0\n),\n \n        y \n=\n \nc\n(\n0\n,\n yCoords\n[,\n2\n],\n \n0\n,\n \n0\n),\n \n        col \n=\n \nfirebrick3\n)\n\n\n# Risk set\n\npolygon\n(\nx \n=\n \nc\n(\n0\n,\n ftime\n[\nord\n],\n ftime\n[\nrev\n(\nord\n)],\n \n0\n,\n \n0\n),\n\n        y \n=\n \nc\n(\n0\n,\n yCoords\n[,\n2\n],\n \nrev\n(\nyCoords\n[,\n2\n]\n \n+\n yCoords\n[,\n4\n]),\n nobs\n,\n \n0\n),\n \n        col \n=\n \ngrey90\n)\n\n\n# Competing event\n\npolygon\n(\nx \n=\n \nc\n(\n0\n,\n ftime\n[\nord\n],\n \nmax\n(\nftime\n),\n \n0\n),\n \n        y \n=\n \nc\n(\nnobs\n,\n nobs \n-\n yCoords\n[,\n1\n],\n nobs\n,\n nobs\n),\n \n        col \n=\n \ndodgerblue2\n)\n\n\n\n# randomly move the cases vertically\n\nmoved_cases \n-\n yCoords\n[\ncases\n[\nord\n],\n \n2\n]\n \n+\n yCoords\n[\ncases\n[\nord\n],\n \n4\n]\n \n*\n runif\n(\nsum\n(\ncases\n))\n\npoints\n((\nftime\n[\nord\n])[\ncases\n[\nord\n]],\n moved_cases\n,\n pch \n=\n \n20\n,\n\n       col \n=\n \nred\n,\n cex \n=\n \n1\n)\n\nlegend\n(\ntopright\n,\n legend \n=\n \nc\n(\nRelapse\n,\n \nCompeting event\n,\n \nAt-risk\n),\n \n       col \n=\n \nc\n(\nfirebrick3\n,\n \ndodgerblue2\n,\n \ngrey90\n),\n\n       pch \n=\n \n15\n)\n\n\n\n\n\n\n\n\nFrom this last plot, we can see that there is no censoring during the first 10 months. Moreover, we see that the last competing event occurs around 20 months. Putting all this information together, we have evidence of two types of patients: very sick patients who either relapse or have a competing event early on, and healthier patients who are eventually lost to follow-up.\n\n\nAnalysis\n\n\nWe now turn to the analysis of this dataset. The population-time plots above give evidence of non-constant hazard; therefore, we will explicitely include time in the model. Note that we also include all other variables as possible confounders. First, we include time as a linear term:\n\n\nmodel1 \n-\n fitSmoothHazard\n(\nStatus \n~\n ftime \n+\n Sex \n+\n D \n+\n Phase \n+\n Source \n+\n Age\n,\n \n                          data \n=\n bmtcrr\n,\n \n                          ratio \n=\n \n1000\n,\n \n                          type \n=\n \nuniform\n,\n \n                          time \n=\n \nftime\n)\n\n\nsummary\n(\nmodel1\n)\n\n\n\n\n\n\n## \n## Call:\n## vglm(formula = formula, family = multinomial(refLevel = 1), data = sampleData)\n## \n## Pearson residuals:\n##                         Min       1Q   Median       3Q   Max\n## log(mu[,2]/mu[,1]) -0.03511 -0.02312 -0.01946 -0.01588 84.80\n## log(mu[,3]/mu[,1]) -0.03693 -0.02680 -0.02337 -0.02017 57.29\n## \n## Coefficients:\n##                  Estimate Std. Error z value Pr(\n|z|)    \n## (Intercept):1  -4.7086629  0.6860839  -6.863 6.74e-12 ***\n## (Intercept):2  -3.7753463  0.4712041  -8.012 1.13e-15 ***\n## ftime:1        -0.0074411  0.0095213  -0.782   0.4345    \n## ftime:2        -0.0234739  0.0118882  -1.975   0.0483 *  \n## SexM:1          0.1173418  0.2766552   0.424   0.6715    \n## SexM:2         -0.1317348  0.2355512  -0.559   0.5760    \n## DAML:1         -0.2615793  0.3039472  -0.861   0.3895    \n## DAML:2          0.1176449  0.2671575   0.440   0.6597    \n## PhaseCR2:1      0.1072881  0.4623358   0.232   0.8165    \n## PhaseCR2:2      0.1036405  0.3310731   0.313   0.7542    \n## PhaseCR3:1      0.2796638  0.6741410   0.415   0.6783    \n## PhaseCR3:2      0.1126718  0.5154568   0.219   0.8270    \n## PhaseRelapse:1  0.8054620  0.3804372   2.117   0.0342 *  \n## PhaseRelapse:2 -0.0007109  0.3000402  -0.002   0.9981    \n## SourcePB:1      0.7456485  0.5311726   1.404   0.1604    \n## SourcePB:2     -0.6471805  0.3288367  -1.968   0.0491 *  \n## Age:1          -0.0151611  0.0118020  -1.285   0.1989    \n## Age:2           0.0186593  0.0098371   1.897   0.0579 .  \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## Number of linear predictors:  2 \n## \n## Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n## \n## Dispersion Parameter for multinomial family:   1\n## \n## Residual deviance: 2221.211 on 262244 degrees of freedom\n## \n## Log-likelihood: -1110.605 on 262244 degrees of freedom\n## \n## Number of iterations: 11 \n## \n## Reference group is level  1  of the response\n\n\n\n\n\nBecause of the results in Turgeon \net al\n \n-@turgeonCompRisk\n, the standard errors we obtain from the multinomial logit fit are asymptotically correct, and therefore can be used to construct asymptotic confidence intervals.\n\n\nFrom this summary, we see that time is indeed significant, as is Phase (only relapse vs. CR1). Interestingly, we see that the type of disease is only significant for the event of interest, whereas the type of transplant and the age of the patient are only significant for the competing event.\n\n\nNext, we include the logarithm of time in the model (which leads to a Weibull hazard):\n\n\nmodel2 \n-\n fitSmoothHazard\n(\nStatus \n~\n \nlog\n(\nftime\n)\n \n+\n Sex \n+\n D \n+\n Phase \n+\n Source \n+\n Age\n,\n \n                          data \n=\n bmtcrr\n,\n \n                          ratio \n=\n \n1000\n,\n \n                          type \n=\n \nuniform\n,\n \n                          time \n=\n \nftime\n)\n\n\nsummary\n(\nmodel2\n)\n\n\n\n\n\n\n## \n## Call:\n## vglm(formula = formula, family = multinomial(refLevel = 1), data = sampleData)\n## \n## Pearson residuals:\n##                         Min       1Q   Median       3Q   Max\n## log(mu[,2]/mu[,1]) -0.05154 -0.02288 -0.01882 -0.01490 79.90\n## log(mu[,3]/mu[,1]) -0.04174 -0.02653 -0.02323 -0.01983 64.38\n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(\n|z|)    \n## (Intercept):1  -5.169337   0.689581  -7.496 6.56e-14 ***\n## (Intercept):2  -4.150096   0.469486  -8.840  \n 2e-16 ***\n## log(ftime):1    0.275468   0.087632   3.143  0.00167 ** \n## log(ftime):2    0.136730   0.074470   1.836  0.06635 .  \n## SexM:1         -0.101932   0.282880  -0.360  0.71860    \n## SexM:2         -0.277052   0.235713  -1.175  0.23984    \n## DAML:1         -0.462982   0.304138  -1.522  0.12794    \n## DAML:2          0.022211   0.273592   0.081  0.93530    \n## PhaseCR2:1      0.135328   0.463264   0.292  0.77020    \n## PhaseCR2:2      0.183499   0.329272   0.557  0.57733    \n## PhaseCR3:1      0.350681   0.680687   0.515  0.60642    \n## PhaseCR3:2      0.137675   0.516486   0.267  0.78981    \n## PhaseRelapse:1  1.106652   0.389873   2.838  0.00453 ** \n## PhaseRelapse:2  0.304955   0.309689   0.985  0.32477    \n## SourcePB:1      0.705531   0.549936   1.283  0.19952    \n## SourcePB:2     -0.737839   0.339630  -2.172  0.02982 *  \n## Age:1          -0.010259   0.011751  -0.873  0.38264    \n## Age:2           0.021852   0.009808   2.228  0.02587 *  \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## Number of linear predictors:  2 \n## \n## Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n## \n## Dispersion Parameter for multinomial family:   1\n## \n## Residual deviance: 2214.089 on 262244 degrees of freedom\n## \n## Log-likelihood: -1107.044 on 262244 degrees of freedom\n## \n## Number of iterations: 10 \n## \n## Reference group is level  1  of the response\n\n\n\n\n\nAs we can see, the results are similar to the ones with a Gompertz hazard, although Sex is now significant for the competing event.\n\n\nFinally, using splines, we can be quite flexible about the way the hazard depends on time:\n\n\nmodel3 \n-\n fitSmoothHazard\n(\n\n    Status \n~\n splines\n::\nbs\n(\nftime\n)\n \n+\n Sex \n+\n D \n+\n Phase \n+\n Source \n+\n Age\n,\n \n    data \n=\n bmtcrr\n,\n \n    ratio \n=\n \n1000\n,\n \n    type \n=\n \nuniform\n,\n \n    time \n=\n \nftime\n)\n\n\nsummary\n(\nmodel3\n)\n\n\n\n\n\n\n## \n## Call:\n## vglm(formula = formula, family = multinomial(refLevel = 1), data = sampleData)\n## \n## Pearson residuals:\n##                         Min       1Q   Median       3Q   Max\n## log(mu[,2]/mu[,1]) -0.05787 -0.02362 -0.01828 -0.01349 103.9\n## log(mu[,3]/mu[,1]) -0.05795 -0.02821 -0.02077 -0.01506  93.3\n## \n## Coefficients:\n##                         Estimate Std. Error z value Pr(\n|z|)    \n## (Intercept):1          -5.322353   0.701356  -7.589 3.23e-14 ***\n## (Intercept):2          -4.806549   0.505056  -9.517  \n 2e-16 ***\n## splines::bs(ftime)1:1  10.051640   2.514058   3.998 6.38e-05 ***\n## splines::bs(ftime)1:2  18.542863   3.706641   5.003 5.66e-07 ***\n## splines::bs(ftime)2:1 -27.155285   9.621508  -2.822 0.004767 ** \n## splines::bs(ftime)2:2 -99.768340  26.045064  -3.831 0.000128 ***\n## splines::bs(ftime)3:1   2.032730   7.508245   0.271 0.786596    \n## splines::bs(ftime)3:2   0.716355  21.435757   0.033 0.973341    \n## SexM:1                  0.010810   0.277099   0.039 0.968881    \n## SexM:2                 -0.224871   0.236840  -0.949 0.342385    \n## DAML:1                 -0.414197   0.302615  -1.369 0.171085    \n## DAML:2                 -0.005705   0.265368  -0.021 0.982847    \n## PhaseCR2:1              0.040206   0.463103   0.087 0.930815    \n## PhaseCR2:2              0.121606   0.330203   0.368 0.712666    \n## PhaseCR3:1              0.535662   0.679935   0.788 0.430805    \n## PhaseCR3:2              0.274984   0.518349   0.531 0.595765    \n## PhaseRelapse:1          1.086171   0.391777   2.772 0.005564 ** \n## PhaseRelapse:2          0.328237   0.310983   1.055 0.291205    \n## SourcePB:1              0.507192   0.537842   0.943 0.345674    \n## SourcePB:2             -0.892167   0.336761  -2.649 0.008067 ** \n## Age:1                  -0.012264   0.012064  -1.017 0.309381    \n## Age:2                   0.022048   0.009932   2.220 0.026427 *  \n## ---\n## Signif. codes:  0 \n***\n 0.001 \n**\n 0.01 \n*\n 0.05 \n.\n 0.1 \n \n 1\n## \n## Number of linear predictors:  2 \n## \n## Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n## \n## Dispersion Parameter for multinomial family:   1\n## \n## Residual deviance: 2157.524 on 262240 degrees of freedom\n## \n## Log-likelihood: -1078.762 on 262240 degrees of freedom\n## \n## Number of iterations: 16 \n## \n## Reference group is level  1  of the response\n\n\n\n\n\nAgain, we see that the results are quite similar for this third model.\n\n\nAbsolute risk\n\n\nWe now look at the 2-year risk of relapse:\n\n\nlinearRisk \n-\n absoluteRisk\n(\nobject \n=\n model1\n,\n time \n=\n \n24\n,\n newdata \n=\n bmtcrr\n[\n1\n:\n10\n,])\n\nlogRisk \n-\n absoluteRisk\n(\nobject \n=\n model2\n,\n time \n=\n \n24\n,\n newdata \n=\n bmtcrr\n[\n1\n:\n10\n,])\n\nsplineRisk \n-\n absoluteRisk\n(\nobject \n=\n model3\n,\n time \n=\n \n24\n,\n newdata \n=\n bmtcrr\n[\n1\n:\n10\n,])\n\n\n\n\n\n\nplot\n(\nlinearRisk\n[,\n1\n],\n logRisk\n[,\n1\n],\n\n     xlab \n=\n \nLinear\n,\n ylab \n=\n \nLog/Spline\n,\n pch \n=\n \n19\n,\n\n     xlim \n=\n \nc\n(\n0\n,\n1\n),\n ylim \n=\n \nc\n(\n0\n,\n1\n),\n col \n=\n \nred\n)\n\npoints\n(\nlinearRisk\n[,\n1\n],\n splineRisk\n[,\n1\n],\n\n       col \n=\n \nblue\n,\n pch \n=\n \n19\n)\n\nabline\n(\na \n=\n \n0\n,\n b \n=\n \n1\n,\n lty \n=\n \n2\n,\n lwd \n=\n \n2\n)\n\nlegend\n(\ntopleft\n,\n legend \n=\n \nc\n(\nLog\n,\n \nSpline\n),\n\n       pch \n=\n \n19\n,\n col \n=\n \nc\n(\nred\n,\n \nblue\n))\n\n\n\n\n\n\n\n\nAs we can see, Model 1 and Model 2 give different absolute risk predictions, but the linear and the spline model actually give very similar results. We can also estimate the mean absolute risk for the entire dataset:\n\n\n# The first column corresponds to the event of interest\n\n\nmean\n(\nlinearRisk\n[,\n1\n])\n\n\n\n\n\n\n## [1] 0.1422626\n\n\n\n\n\nmean\n(\nlogRisk\n[,\n1\n])\n\n\n\n\n\n\n## [1] 0.1816989\n\n\n\n\n\nmean\n(\nsplineRisk\n[,\n1\n])\n\n\n\n\n\n\n## [1] 0.1393753\n\n\n\n\n\nSession information\n\n\n## R version 3.3.1 (2016-06-21)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.10\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n## [1] casebase_0.1.0\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.9      knitr_1.15.1     magrittr_1.5     splines_3.3.1   \n##  [5] munsell_0.4.3    lattice_0.20-33  colorspace_1.3-1 stringr_1.2.0   \n##  [9] plyr_1.8.4       tools_3.3.1      grid_3.3.1       data.table_1.9.6\n## [13] gtable_0.2.0     htmltools_0.3.5  survival_2.39-5  yaml_2.1.14     \n## [17] lazyeval_0.2.0   rprojroot_1.2    digest_0.6.12    assertthat_0.1  \n## [21] tibble_1.2       Matrix_1.2-6     ggplot2_2.2.0    codetools_0.2-14\n## [25] VGAM_1.0-2       evaluate_0.10    rmarkdown_1.3    stringi_1.1.2   \n## [29] scales_0.4.1     backports_1.0.5  stats4_3.3.1     chron_2.3-47\n\n\n\n\n\nReferences", 
            "title": "Competing Risk"
        }, 
        {
            "location": "/competingRisk/#data", 
            "text": "We will use the same data that was used in Scrucca  et al   -@scrucca2010regression . The data is available on the main author's  website .  set.seed ( 12345 )  library ( casebase ) \ndata ( bmtcrr )  head ( bmtcrr )   ##   Sex   D   Phase Age Status Source  ftime\n## 1   M ALL Relapse  48      2  BM+PB   0.67\n## 2   F AML     CR2  23      1  BM+PB   9.50\n## 3   M ALL     CR3   7      0  BM+PB 131.77\n## 4   F ALL     CR2  26      2  BM+PB  24.03\n## 5   F ALL     CR2  36      2  BM+PB   1.47\n## 6   M ALL Relapse  17      2  BM+PB   2.23  We will perform a competing risk analysis on data from 177 patients who received a stem cell transplant for acute leukemia. The event of interest in relapse, but other competing causes (e.g. transplant-related death) need to be taken into account. We also want to take into account the effect of several covariates such as Sex, Disease (lymphoblastic or myeloblastic leukemia, abbreviated as ALL and AML, respectively), Phase at transplant (Relapse, CR1, CR2, CR3), Source of stem cells (bone marrow and peripheral blood, coded as BM+PB, or peripheral blood, coded as PB), and Age. Below, we reproduce their Table 1:          Variable  Description  Statistical summary      Sex  Sex  M=Male (100)   F=Female (77)    D  Disease  ALL (73)   AML (104)    Phase  Phase  CR1 (47)   CR2 (45)   CR3 (12)   Relapse (73)    Source  Type of transplant  BM+PB (21)   PB (156)    Age  Age of patient (years)  4\u201362   30.47 (13.04)    Ftime  Failure time (months)  0.13\u2013131.77   20.28 (30.78)    Status  Status indicator  0=censored (46)   1=relapse (56)   2=competing event (75)     The statistical summary is generated differently for continuous and categorical variables:    For continuous variables, we are given the range, followed by the mean and standard deviation.    For categorical variables, we are given the counts for each category.    Note that failure time can also correspond to censoring.", 
            "title": "Data"
        }, 
        {
            "location": "/competingRisk/#population-time-plots", 
            "text": "In order to try and visualize the incidence density of relapse, we can look at a population-time plot: on the X-axis we have time, and on the Y-axis we have the size of the risk set at a particular time point. Failure times associated to the event of interest can then be highlighted on the plot using red dots.  nobs  -   nrow ( bmtcrr ) \nftime  -  bmtcrr $ ftime\nord  -   order ( ftime ,  decreasing  =   FALSE )  # We split the person-moments in four categories:  # 1) at-risk  # 2) main event  # 3) competing event  # 4) censored \nyCoords  -   cbind ( cumsum ( bmtcrr [ ord ,   Status ]   ==   2 ),  \n                  cumsum ( bmtcrr [ ord ,   Status ]   ==   1 ), \n                  cumsum ( bmtcrr [ ord ,   Status ]   ==   0 )) \nyCoords  -   cbind ( yCoords ,  nobs  -   rowSums ( yCoords ))  # Plot only at-risk \nplot ( 0 ,  type  =   n ,  xlim  =   c ( 0 ,   max ( ftime )),  ylim  =   c ( 0 ,  nobs ),  \n     xlab  =   Follow-up time ,  ylab  =   Population ) \npolygon ( c ( 0 ,   0 ,  ftime [ ord ],   max ( ftime ),   0 ), \n         c ( 0 ,  nobs ,  yCoords [, 4 ],   0 ,   0 ),  col  =   grey90 ) \ncases  -  bmtcrr [,   Status ]   ==   1  # randomly move the cases vertically \nmoved_cases  -  yCoords [ cases [ ord ],   4 ]   *  runif ( sum ( cases )) \npoints (( ftime [ ord ])[ cases [ ord ]],  moved_cases ,  pch  =   20 ,  \n       col  =   red ,  cex  =   1 )    We can right away draw a few conclusions from this plot: first of all, we get a sense of how quickly the size of the risk set changes over time. We also see that the incidence density is non-constant: most relapses occur before 15 months. Finally, we also see that the risk set keeps shrinking after the last event has occured; this could be due to either censoring or the competing event.  To get an idea of whether only relapse is responsible for the shrinking of the risk set in the first few months of follow-up, we can also keep track of how many events have occured at each time point:  # Plot at-risk and events \nplot ( 0 ,  type  =   n ,  xlim  =   c ( 0 ,   max ( ftime )),  ylim  =   c ( 0 ,  nobs ),  \n     xlab  =   Follow-up time ,  ylab  =   Population ) \npolygon ( x  =   c ( 0 , ftime [ ord ],   max ( ftime ),   0 ),  \n        y  =   c ( 0 ,  yCoords [, 2 ],   0 ,   0 ),  \n        col  =   firebrick3 ) \npolygon ( x  =   c ( 0 ,  ftime [ ord ],  ftime [ rev ( ord )],   0 ,   0 ), \n        y  =   c ( 0 ,  yCoords [, 2 ],   rev ( yCoords [, 2 ]   +  yCoords [, 4 ]),  nobs ,   0 ),  \n        col  =   grey90 )  # randomly move the cases vertically \nmoved_cases  -  yCoords [ cases [ ord ],   2 ]   +  yCoords [ cases [ ord ],   4 ]   *  runif ( sum ( cases )) \npoints (( ftime [ ord ])[ cases [ ord ]],  moved_cases ,  pch  =   20 , \n       col  =   red ,  cex  =   1 ) \nlegend ( topright ,  legend  =   c ( Relapse ,   At-risk ),  \n       col  =   c ( firebrick3 ,   grey90 ), \n       pch  =   15 )    Therefore, there is also censoring and loss due to competing events happening in the first few months. However, with this plot, we can't differentiate bwetween the two contributions. For this reason we can also keep track of the number of competing events at each time point:  plot ( 0 ,  type  =   n ,  xlim  =   c ( 0 ,   max ( ftime )),  ylim  =   c ( 0 ,  nobs ),  \n     xlab  =   Follow-up time ,  ylab  =   Population ) \npolygon ( x  =   c ( 0 ,   max ( ftime ),   max ( ftime ),   0 ), \n        y  =   c ( 0 ,   0 ,  nobs ,  nobs ),  col  =   white )  # Event of interest \npolygon ( x  =   c ( 0 , ftime [ ord ],   max ( ftime ),   0 ),  \n        y  =   c ( 0 ,  yCoords [, 2 ],   0 ,   0 ),  \n        col  =   firebrick3 )  # Risk set \npolygon ( x  =   c ( 0 ,  ftime [ ord ],  ftime [ rev ( ord )],   0 ,   0 ), \n        y  =   c ( 0 ,  yCoords [, 2 ],   rev ( yCoords [, 2 ]   +  yCoords [, 4 ]),  nobs ,   0 ),  \n        col  =   grey90 )  # Competing event \npolygon ( x  =   c ( 0 ,  ftime [ ord ],   max ( ftime ),   0 ),  \n        y  =   c ( nobs ,  nobs  -  yCoords [, 1 ],  nobs ,  nobs ),  \n        col  =   dodgerblue2 )  # randomly move the cases vertically \nmoved_cases  -  yCoords [ cases [ ord ],   2 ]   +  yCoords [ cases [ ord ],   4 ]   *  runif ( sum ( cases )) \npoints (( ftime [ ord ])[ cases [ ord ]],  moved_cases ,  pch  =   20 , \n       col  =   red ,  cex  =   1 ) \nlegend ( topright ,  legend  =   c ( Relapse ,   Competing event ,   At-risk ),  \n       col  =   c ( firebrick3 ,   dodgerblue2 ,   grey90 ), \n       pch  =   15 )    From this last plot, we can see that there is no censoring during the first 10 months. Moreover, we see that the last competing event occurs around 20 months. Putting all this information together, we have evidence of two types of patients: very sick patients who either relapse or have a competing event early on, and healthier patients who are eventually lost to follow-up.", 
            "title": "Population-time plots"
        }, 
        {
            "location": "/competingRisk/#analysis", 
            "text": "We now turn to the analysis of this dataset. The population-time plots above give evidence of non-constant hazard; therefore, we will explicitely include time in the model. Note that we also include all other variables as possible confounders. First, we include time as a linear term:  model1  -  fitSmoothHazard ( Status  ~  ftime  +  Sex  +  D  +  Phase  +  Source  +  Age ,  \n                          data  =  bmtcrr ,  \n                          ratio  =   1000 ,  \n                          type  =   uniform ,  \n                          time  =   ftime )  summary ( model1 )   ## \n## Call:\n## vglm(formula = formula, family = multinomial(refLevel = 1), data = sampleData)\n## \n## Pearson residuals:\n##                         Min       1Q   Median       3Q   Max\n## log(mu[,2]/mu[,1]) -0.03511 -0.02312 -0.01946 -0.01588 84.80\n## log(mu[,3]/mu[,1]) -0.03693 -0.02680 -0.02337 -0.02017 57.29\n## \n## Coefficients:\n##                  Estimate Std. Error z value Pr( |z|)    \n## (Intercept):1  -4.7086629  0.6860839  -6.863 6.74e-12 ***\n## (Intercept):2  -3.7753463  0.4712041  -8.012 1.13e-15 ***\n## ftime:1        -0.0074411  0.0095213  -0.782   0.4345    \n## ftime:2        -0.0234739  0.0118882  -1.975   0.0483 *  \n## SexM:1          0.1173418  0.2766552   0.424   0.6715    \n## SexM:2         -0.1317348  0.2355512  -0.559   0.5760    \n## DAML:1         -0.2615793  0.3039472  -0.861   0.3895    \n## DAML:2          0.1176449  0.2671575   0.440   0.6597    \n## PhaseCR2:1      0.1072881  0.4623358   0.232   0.8165    \n## PhaseCR2:2      0.1036405  0.3310731   0.313   0.7542    \n## PhaseCR3:1      0.2796638  0.6741410   0.415   0.6783    \n## PhaseCR3:2      0.1126718  0.5154568   0.219   0.8270    \n## PhaseRelapse:1  0.8054620  0.3804372   2.117   0.0342 *  \n## PhaseRelapse:2 -0.0007109  0.3000402  -0.002   0.9981    \n## SourcePB:1      0.7456485  0.5311726   1.404   0.1604    \n## SourcePB:2     -0.6471805  0.3288367  -1.968   0.0491 *  \n## Age:1          -0.0151611  0.0118020  -1.285   0.1989    \n## Age:2           0.0186593  0.0098371   1.897   0.0579 .  \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## Number of linear predictors:  2 \n## \n## Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n## \n## Dispersion Parameter for multinomial family:   1\n## \n## Residual deviance: 2221.211 on 262244 degrees of freedom\n## \n## Log-likelihood: -1110.605 on 262244 degrees of freedom\n## \n## Number of iterations: 11 \n## \n## Reference group is level  1  of the response  Because of the results in Turgeon  et al   -@turgeonCompRisk , the standard errors we obtain from the multinomial logit fit are asymptotically correct, and therefore can be used to construct asymptotic confidence intervals.  From this summary, we see that time is indeed significant, as is Phase (only relapse vs. CR1). Interestingly, we see that the type of disease is only significant for the event of interest, whereas the type of transplant and the age of the patient are only significant for the competing event.  Next, we include the logarithm of time in the model (which leads to a Weibull hazard):  model2  -  fitSmoothHazard ( Status  ~   log ( ftime )   +  Sex  +  D  +  Phase  +  Source  +  Age ,  \n                          data  =  bmtcrr ,  \n                          ratio  =   1000 ,  \n                          type  =   uniform ,  \n                          time  =   ftime )  summary ( model2 )   ## \n## Call:\n## vglm(formula = formula, family = multinomial(refLevel = 1), data = sampleData)\n## \n## Pearson residuals:\n##                         Min       1Q   Median       3Q   Max\n## log(mu[,2]/mu[,1]) -0.05154 -0.02288 -0.01882 -0.01490 79.90\n## log(mu[,3]/mu[,1]) -0.04174 -0.02653 -0.02323 -0.01983 64.38\n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr( |z|)    \n## (Intercept):1  -5.169337   0.689581  -7.496 6.56e-14 ***\n## (Intercept):2  -4.150096   0.469486  -8.840    2e-16 ***\n## log(ftime):1    0.275468   0.087632   3.143  0.00167 ** \n## log(ftime):2    0.136730   0.074470   1.836  0.06635 .  \n## SexM:1         -0.101932   0.282880  -0.360  0.71860    \n## SexM:2         -0.277052   0.235713  -1.175  0.23984    \n## DAML:1         -0.462982   0.304138  -1.522  0.12794    \n## DAML:2          0.022211   0.273592   0.081  0.93530    \n## PhaseCR2:1      0.135328   0.463264   0.292  0.77020    \n## PhaseCR2:2      0.183499   0.329272   0.557  0.57733    \n## PhaseCR3:1      0.350681   0.680687   0.515  0.60642    \n## PhaseCR3:2      0.137675   0.516486   0.267  0.78981    \n## PhaseRelapse:1  1.106652   0.389873   2.838  0.00453 ** \n## PhaseRelapse:2  0.304955   0.309689   0.985  0.32477    \n## SourcePB:1      0.705531   0.549936   1.283  0.19952    \n## SourcePB:2     -0.737839   0.339630  -2.172  0.02982 *  \n## Age:1          -0.010259   0.011751  -0.873  0.38264    \n## Age:2           0.021852   0.009808   2.228  0.02587 *  \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## Number of linear predictors:  2 \n## \n## Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n## \n## Dispersion Parameter for multinomial family:   1\n## \n## Residual deviance: 2214.089 on 262244 degrees of freedom\n## \n## Log-likelihood: -1107.044 on 262244 degrees of freedom\n## \n## Number of iterations: 10 \n## \n## Reference group is level  1  of the response  As we can see, the results are similar to the ones with a Gompertz hazard, although Sex is now significant for the competing event.  Finally, using splines, we can be quite flexible about the way the hazard depends on time:  model3  -  fitSmoothHazard ( \n    Status  ~  splines :: bs ( ftime )   +  Sex  +  D  +  Phase  +  Source  +  Age ,  \n    data  =  bmtcrr ,  \n    ratio  =   1000 ,  \n    type  =   uniform ,  \n    time  =   ftime )  summary ( model3 )   ## \n## Call:\n## vglm(formula = formula, family = multinomial(refLevel = 1), data = sampleData)\n## \n## Pearson residuals:\n##                         Min       1Q   Median       3Q   Max\n## log(mu[,2]/mu[,1]) -0.05787 -0.02362 -0.01828 -0.01349 103.9\n## log(mu[,3]/mu[,1]) -0.05795 -0.02821 -0.02077 -0.01506  93.3\n## \n## Coefficients:\n##                         Estimate Std. Error z value Pr( |z|)    \n## (Intercept):1          -5.322353   0.701356  -7.589 3.23e-14 ***\n## (Intercept):2          -4.806549   0.505056  -9.517    2e-16 ***\n## splines::bs(ftime)1:1  10.051640   2.514058   3.998 6.38e-05 ***\n## splines::bs(ftime)1:2  18.542863   3.706641   5.003 5.66e-07 ***\n## splines::bs(ftime)2:1 -27.155285   9.621508  -2.822 0.004767 ** \n## splines::bs(ftime)2:2 -99.768340  26.045064  -3.831 0.000128 ***\n## splines::bs(ftime)3:1   2.032730   7.508245   0.271 0.786596    \n## splines::bs(ftime)3:2   0.716355  21.435757   0.033 0.973341    \n## SexM:1                  0.010810   0.277099   0.039 0.968881    \n## SexM:2                 -0.224871   0.236840  -0.949 0.342385    \n## DAML:1                 -0.414197   0.302615  -1.369 0.171085    \n## DAML:2                 -0.005705   0.265368  -0.021 0.982847    \n## PhaseCR2:1              0.040206   0.463103   0.087 0.930815    \n## PhaseCR2:2              0.121606   0.330203   0.368 0.712666    \n## PhaseCR3:1              0.535662   0.679935   0.788 0.430805    \n## PhaseCR3:2              0.274984   0.518349   0.531 0.595765    \n## PhaseRelapse:1          1.086171   0.391777   2.772 0.005564 ** \n## PhaseRelapse:2          0.328237   0.310983   1.055 0.291205    \n## SourcePB:1              0.507192   0.537842   0.943 0.345674    \n## SourcePB:2             -0.892167   0.336761  -2.649 0.008067 ** \n## Age:1                  -0.012264   0.012064  -1.017 0.309381    \n## Age:2                   0.022048   0.009932   2.220 0.026427 *  \n## ---\n## Signif. codes:  0  ***  0.001  **  0.01  *  0.05  .  0.1     1\n## \n## Number of linear predictors:  2 \n## \n## Names of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n## \n## Dispersion Parameter for multinomial family:   1\n## \n## Residual deviance: 2157.524 on 262240 degrees of freedom\n## \n## Log-likelihood: -1078.762 on 262240 degrees of freedom\n## \n## Number of iterations: 16 \n## \n## Reference group is level  1  of the response  Again, we see that the results are quite similar for this third model.", 
            "title": "Analysis"
        }, 
        {
            "location": "/competingRisk/#absolute-risk", 
            "text": "We now look at the 2-year risk of relapse:  linearRisk  -  absoluteRisk ( object  =  model1 ,  time  =   24 ,  newdata  =  bmtcrr [ 1 : 10 ,]) \nlogRisk  -  absoluteRisk ( object  =  model2 ,  time  =   24 ,  newdata  =  bmtcrr [ 1 : 10 ,]) \nsplineRisk  -  absoluteRisk ( object  =  model3 ,  time  =   24 ,  newdata  =  bmtcrr [ 1 : 10 ,])   plot ( linearRisk [, 1 ],  logRisk [, 1 ], \n     xlab  =   Linear ,  ylab  =   Log/Spline ,  pch  =   19 , \n     xlim  =   c ( 0 , 1 ),  ylim  =   c ( 0 , 1 ),  col  =   red ) \npoints ( linearRisk [, 1 ],  splineRisk [, 1 ], \n       col  =   blue ,  pch  =   19 ) \nabline ( a  =   0 ,  b  =   1 ,  lty  =   2 ,  lwd  =   2 ) \nlegend ( topleft ,  legend  =   c ( Log ,   Spline ), \n       pch  =   19 ,  col  =   c ( red ,   blue ))    As we can see, Model 1 and Model 2 give different absolute risk predictions, but the linear and the spline model actually give very similar results. We can also estimate the mean absolute risk for the entire dataset:  # The first column corresponds to the event of interest  mean ( linearRisk [, 1 ])   ## [1] 0.1422626  mean ( logRisk [, 1 ])   ## [1] 0.1816989  mean ( splineRisk [, 1 ])   ## [1] 0.1393753", 
            "title": "Absolute risk"
        }, 
        {
            "location": "/competingRisk/#session-information", 
            "text": "## R version 3.3.1 (2016-06-21)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.10\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n## [1] casebase_0.1.0\n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.9      knitr_1.15.1     magrittr_1.5     splines_3.3.1   \n##  [5] munsell_0.4.3    lattice_0.20-33  colorspace_1.3-1 stringr_1.2.0   \n##  [9] plyr_1.8.4       tools_3.3.1      grid_3.3.1       data.table_1.9.6\n## [13] gtable_0.2.0     htmltools_0.3.5  survival_2.39-5  yaml_2.1.14     \n## [17] lazyeval_0.2.0   rprojroot_1.2    digest_0.6.12    assertthat_0.1  \n## [21] tibble_1.2       Matrix_1.2-6     ggplot2_2.2.0    codetools_0.2-14\n## [25] VGAM_1.0-2       evaluate_0.10    rmarkdown_1.3    stringi_1.1.2   \n## [29] scales_0.4.1     backports_1.0.5  stats4_3.3.1     chron_2.3-47", 
            "title": "Session information"
        }, 
        {
            "location": "/competingRisk/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\n\n\n\n\n\nEfron, Bradley. 1977. \"The Efficiency of Cox's Likelihood Function for Censored Data.\" \nJournal of the American Statistical Association\n 72 (359). Taylor \n&\n Francis Group: 557\u201365.\n\n\n\n\n\n\n\n\nHanley, James A, and Olli S Miettinen. 2009. \"Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression.\" \nThe International Journal of Biostatistics\n 5 (1).\n\n\n\n\n\n\n\n\nMantel, Nathan. 1973. \"Synthetic Retrospective Studies and Related Topics.\" \nBiometrics\n. JSTOR, 479\u201386.\n\n\n\n\n\n\n\n\nSaarela, Olli. 2015. \"A Case-Base Sampling Method for Estimating Recurrent Event Intensities.\" \nLifetime Data Analysis\n. Springer, 1\u201317.\n\n\n\n\n\n\n\n\nSaarela, Olli, and Elja Arjas. 2015. \"Non-Parametric Bayesian Hazard Regression for Chronic Disease Risk Assessment.\" \nScandinavian Journal of Statistics\n 42 (2). Wiley Online Library: 609\u201326.\n\n\n\n\n\n\n\n\nScrucca, L, A Santucci, and F Aversa. 2010. \"Regression Modeling of Competing Risk Using R: An in Depth Guide for Clinicians.\" \nBone Marrow Transplantation\n 45 (9). Nature Publishing Group: 1388\u201395.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "Efron, Bradley. 1977. \"The Efficiency of Cox's Likelihood Function for Censored Data.\"  Journal of the American Statistical Association  72 (359). Taylor  &  Francis Group: 557\u201365.    \nHanley, James A, and Olli S Miettinen. 2009. \"Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression.\"  The International Journal of Biostatistics  5 (1).    \nMantel, Nathan. 1973. \"Synthetic Retrospective Studies and Related Topics.\"  Biometrics . JSTOR, 479\u201386.    \nSaarela, Olli. 2015. \"A Case-Base Sampling Method for Estimating Recurrent Event Intensities.\"  Lifetime Data Analysis . Springer, 1\u201317.    \nSaarela, Olli, and Elja Arjas. 2015. \"Non-Parametric Bayesian Hazard Regression for Chronic Disease Risk Assessment.\"  Scandinavian Journal of Statistics  42 (2). Wiley Online Library: 609\u201326.    \nScrucca, L, A Santucci, and F Aversa. 2010. \"Regression Modeling of Competing Risk Using R: An in Depth Guide for Clinicians.\"  Bone Marrow Transplantation  45 (9). Nature Publishing Group: 1388\u201395.", 
            "title": "References"
        }
    ]
}