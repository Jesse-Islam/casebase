---
title: "data"
author: "Sahir"
date: "January 22, 2017"
output:
  md_document:
    variant: markdown
---

## casebase: An Alternative Framework for Survival Analysis

This vignette introduces the main functions in the `casebase` package. The methods implemented in this package are based on the method developped in [Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression (Hanley and Miettinen, 2009)](https://github.com/sahirbhatnagar/casebase/blob/master/references/Hanley_Miettinen-2009-Inter_J_of_Biostats.pdf). A rigorous treatment of the theory is developed in [A case-base sampling method for estimating recurrent
event intensities (Saarela, 2015)](https://github.com/sahirbhatnagar/casebase/blob/master/references/Saarela-2015-Lifetime_Data_Analysis.pdf) and [Non-parametric Bayesian Hazard Regression for Chronic Disease Risk Assessment](https://github.com/sahirbhatnagar/casebase/blob/master/references/Saarela_et_al-2015-Scandinavian_Journal_of_Statistics.pdf). The motivation for this work is nicely summarised by Cox:

![](http://i.imgur.com/E674HYw.png)

### Why another package for survival analysis?

The purpose of the `casebase` package is to provide practitioners with an easy-to-use software tool to predict the risk (or cumulative incidence (CI)) of an event, for a particular patient. The following points should be noted:

1. Time matching/risk set sampling (including Cox partial likelihood) eliminates the baseline hazard from the likelihood expression for the
hazard ratios
2. If, however, the absolute risks are of interest, they have to be recovered using the semi-parametric Breslow estimator
3. Alternative approaches for fitting flexible hazard models for estimating absolute risks, not requiring this two-step approach? Yes! [Hanley and Miettinen, 2009](https://github.com/sahirbhatnagar/casebase/blob/master/references/Hanley_Miettinen-2009-Inter_J_of_Biostats.pdf)  

> [Hanley and Miettinen, 2009](https://github.com/sahirbhatnagar/casebase/blob/master/references/Hanley_Miettinen-2009-Inter_J_of_Biostats.pdf) propose a fully parametric hazard model that can be fit via logistic regression. From the fitted hazard function, cumulative incidence and, thus, risk functions of time, treatment and profile can be easily derived.


### Parametric family of hazard functions

The `casebase` package fits the family of hazard functions of the form

$$ h(x,t) = exp[g(x,t)] $$
where \\( t \\) denotes the numerical value (number of units) of a point in prognostic/prospective time and \\( x \\) is the realization of the vector \\( X \\) of variates based on the patient's profile and intervention (if any). Different functions of \\( t \\) lead to different parametric hazard models. 

The simplest of these models is the one-parameter exponential distribution which is obtained by taking the hazard function to be constant over the range of \\( t \\). 

$$ h(x,t) = exp(\beta_0 + \beta_1 x) $$

The instantaneous failure rate is independent of \\( t \\), so that the conditional chance of failure in a time interval of specified length is the same regardless of how long the individual has been on study a.k.a the memoryless property (Kalbfleisch and Prentice, 2002).


The Gompertz hazard model is given by including a linear term for time:

$$ h(x,t)  = exp(\beta_0 + \beta_1 t + \beta_2 x) $$

Use of \\( log(t) \\) yields the Weibull hazard which allows for a power dependence of the hazard on time (Kalbfleisch and Prentice, 2002):

$$ h(x, t)  = exp(\beta_0 + \beta_1 \log(t) + \beta_2 x) $$

Recall that the relative risk model (Cox, 1972)

$$ \lambda(t;x) = \lambda_0(t) exp(\mathbf{X}\boldsymbol{\beta})  $$
where \\( \lambda_0(\cdot)  \\) is an arbitrary unspecified baseline hazard function for continous time. 


## Cox Model vs. Case-base Sampling

In the following table we provide a comparison between the Cox model and case-base sampling:

```{r, echo=FALSE, results='asis'}
knitr::kable(data.frame(`feature` = c("model type", "time", "cumulative incidence", "non-proportional hazards","model testing","competing risks", "prediction"),
           `Cox` = c("semi-parametric","left hand side of the equation", "step function", "interaction of covariates with time"," ", "difficult", "Kaplan-Meier-based"),
           `Case Base Sampling` = c("fully parametric (logistic/multinomial regression)", "right hand side - allows flexible modeling of time",
           "smooth-in-time curve", "interaction of covariates with time", "make use of GLM framework (LRT, AIC, BIC)",
           "cause-specific cumulative incidence functions (CIFs) directly obtained via multinomial regression", "ROC, AUC, risk reclassification probabilities")), format = "html", escape = FALSE)
```

## Intuition Behind Casebase sampling

[Slides from Olli Saarela](https://www.fields.utoronto.ca/programs/scientific/14-15/biomarker/slides/saarela.pdf)

## Load Required Packages

We fist load the required packages:

```{r setup, echo=c(-1,-2), message=FALSE, warning=FALSE}
rm(list=ls())
options(digits = 2, scipen = 999)
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(survival)
pacman::p_load(casebase)
pacman::p_load(splines)
```


## European Randomized Study of Prostate Cancer Screening Data

Throughout this vignette, we make use of the European Randomized Study of Prostate Cancer Screening data which ships with the `casebase` package:

```{r}
data("ERSPC")
head(ERSPC)
ERSPC$ScrArm <- factor(ERSPC$ScrArm, 
                       levels = c(0,1), 
                       labels = c("Control group", "Screening group"))
```

The results of this study were published by [Schroder FH, et al. N Engl J Med 2009](https://github.com/sahirbhatnagar/casebase/blob/master/references/Schroder_et_al-2009-NEJM.pdf). There's a really interesting story on how this data was obtained. See `help(ERSPC)` and [Liu Z, Rich B, Hanley JA, Recovering the raw data behind a non-parametric survival curve. Systematic Reviews 2014](https://github.com/sahirbhatnagar/casebase/blob/master/references/Liu_et_al-2015-Systematic_Reviews.pdf) for further details.

## Population Time Plot

Population time plots can be extremely informative graphical displays of survival data. They should be the first step in your exploratory data analyses. We facilitate this task in the `casebase` package using the `popTime` function. We first create the necessary dataset for producing the population time plots:

```{r}
pt_object <- casebase::popTime(ERSPC, event = "DeadOfPrCa")
```

We can see its contents and its class:

```{r}
head(pt_object)

class(pt_object)
```

The `casebase` package has a `plot` method for objects of class `popTime`:

```{r}
plot(pt_object)
```


> Can you explain the distinct shape of the grey area?



## Exposure Stratified Population Time Plot

We can also create exposure stratified plots by specifying the `exposure` argument in the `popTime` function:

```{r}
pt_object_strat <- casebase::popTime(ERSPC, 
                                     event = "DeadOfPrCa", 
                                     exposure = "ScrArm")
```

We can see its contents and its class:

```{r}
head(pt_object_strat)

class(pt_object_strat)
```

The `casebase` package also has a `plot` method for objects of class `popTimeExposure`:

```{r}
plot(pt_object_strat)
```


We can also plot them side-by-side using the `ncol` argument:

```{r}
plot(pt_object_strat, ncol = 2)
```


## Cox Model

We first fit a Cox model, examine the hazard ratio for the screening group (relative to the control group), and plot the cumulative incidence function (CIF). 

```{r}
cox_model <- survival::coxph(Surv(Follow.Up.Time, DeadOfPrCa) ~ ScrArm, 
                             data = ERSPC)
(sum_cox_model <- summary(cox_model))
```

We can plot the CIF for each group:

```{r}
new_data <- data.frame(ScrArm = c("Control group", "Screening group"),
                       ignore = 99)

plot(survfit(cox_model, newdata=new_data),
     xlab = "Years since Randomization", 
     ylab="Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = F, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    sum_cox_model$conf.int[,"exp(coef)"], 
                    sum_cox_model$conf.int[,"lower .95"], 
                    sum_cox_model$conf.int[,"upper .95"]))
legend("topleft", 
       legend = c("Control group", "Screening group"), 
       col = c("red","blue"),
       lty = c(1, 1), 
       bg = "gray90")
```

We compare it to the figure in [Schroder FH, et al. N Engl J Med 2009](https://github.com/sahirbhatnagar/casebase/blob/master/references/Schroder_et_al-2009-NEJM.pdf) and see that the plots are very similar, as is the hazard ratio and 95% confidence interval:

![](http://i.imgur.com/fk21dlV.png)


## Case-base Sampling

Next we fit several models using case-base sampling. The models we fit differ in how we choose to model time. 

The `fitSmoothHazard` function provides an estimate of the hazard function \\( h(x, t) \\) is the hazard function, \\( t \\) denotes the numerical value (number of units) of a point in prognostic/prospective time and \\( x \\) is the realization of the vector \\( X \\) of variates based on the patient's profile and intervention (if any).

```{r}
# set the seed for reproducible output
set.seed(1234)

casebase_exponential <- casebase::fitSmoothHazard(DeadOfPrCa ~ ScrArm, 
                                                  data = ERSPC, 
                                                  ratio = 100)

summary(casebase_exponential)
exp(casebase_exponential$coefficients[2])
exp(confint(casebase_exponential)[2,])
```

The `absoluteRisk` function provides an estimate of the cumulative incidence curves for a specific risk profile using the following equation:

$$ CI(x, t) = 1 - exp\left[ - \int_0^t h(x, u) \textrm{d}u \right] $$

In the plot below, we overlay the estimated CIF from the casebase exponential model on the Cox model CIF:

```{r}
smooth_risk_exp <- casebase::absoluteRisk(object = casebase_exponential, 
                                          time = seq(0,15,0.1), 
                                          newdata = new_data)

plot(survfit(cox_model, newdata=new_data),
     xlab = "Years since Randomization", 
     ylab="Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = F, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    sum_cox_model$conf.int[,"exp(coef)"], 
                    sum_cox_model$conf.int[,"lower .95"], 
                    sum_cox_model$conf.int[,"upper .95"]))
lines(seq(0,15,0.1), smooth_risk_exp[1,], type = "l", col = "red", lty = 2)
lines(seq(0,15,0.1), smooth_risk_exp[2,], type = "l", col = "blue", lty = 2)


legend("topleft", 
       legend = c("Control group (Cox)","Control group (Casebase)",
                  "Screening group (Cox)", "Screening group (Casebase)"), 
       col = c("red","red", "blue","blue"),
       lty = c(1, 2, 1, 2), 
       bg = "gray90")

```

As we can see, the exponential model is not a good fit. Based on what we observed in the population time plot, where more events are observed later on in time, this poor fit is expected. A constant hazard model would overestimate the cumulative incidence earlier on in time, and underestimate it later on, which is what we see in the cumulative incidence plot. This example demonstrates the benefits of population time plots as an exploratory analysis tool. 

### Linear Time

Next we enter time linearly into the model:

```{r}
casebase_time <- fitSmoothHazard(DeadOfPrCa ~ Follow.Up.Time + ScrArm, 
                                 data = ERSPC, 
                                 ratio = 100)

summary(casebase_time)
exp(casebase_time$coefficients)
exp(confint(casebase_time))
```


```{r}
smooth_risk_time <- casebase::absoluteRisk(object = casebase_time, 
                                          time = seq(0,15,0.1), 
                                          newdata = new_data)

plot(survfit(cox_model, newdata=new_data),
     xlab = "Years since Randomization", 
     ylab="Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = F, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    sum_cox_model$conf.int[,"exp(coef)"], 
                    sum_cox_model$conf.int[,"lower .95"], 
                    sum_cox_model$conf.int[,"upper .95"]))
lines(seq(0,15,0.1), smooth_risk_time[1,], type = "l", col = "red", lty = 2)
lines(seq(0,15,0.1), smooth_risk_time[2,], type = "l", col = "blue", lty = 2)

legend("topleft", 
       legend = c("Control group (Cox)","Control group (Casebase)",
                  "Screening group (Cox)", "Screening group (Casebase)"), 
       col = c("red","red", "blue","blue"),
       lty = c(1, 2, 1, 2), 
       bg = "gray90")

```

We see that the Weibull model leads to a better fit.

### Flexible time using BSplines

Next we try to enter a smooth function of time into the model using the `splines` package

```{r}
casebase_splines <- fitSmoothHazard(DeadOfPrCa ~ bs(Follow.Up.Time) + ScrArm, 
                                    data = ERSPC, 
                                    ratio = 100)

summary(casebase_splines)
exp(casebase_splines$coefficients)
exp(confint(casebase_splines))
```


```{r}
smooth_risk_splines <- absoluteRisk(object = casebase_splines, 
                                    time = seq(0,15,0.1), 
                                    newdata = new_data)

plot(survfit(cox_model, newdata=new_data),
     xlab = "Years since Randomization", 
     ylab="Cumulative Incidence", 
     fun = "event",
     xlim = c(0,15), conf.int = F, col = c("red","blue"), 
     main = sprintf("Estimated Cumulative Incidence (risk) of Death from Prostate 
                    Cancer Screening group Hazard Ratio: %.2g (%.2g, %.2g)",
                    sum_cox_model$conf.int[,"exp(coef)"], 
                    sum_cox_model$conf.int[,"lower .95"], 
                    sum_cox_model$conf.int[,"upper .95"]))
lines(seq(0,15,0.1), smooth_risk_splines[1,], type = "l", col = "red", lty = 2)
lines(seq(0,15,0.1), smooth_risk_splines[2,], type = "l", col = "blue", lty = 2)

legend("topleft", 
       legend = c("Control group (Cox)","Control group (Casebase)",
                  "Screening group (Cox)", "Screening group (Casebase)"), 
       col = c("red","red", "blue","blue"),
       lty = c(1, 2, 1, 2), 
       bg = "gray90")

```

It looks like the best fit. 

## Comparing Models Using Likelihood Ratio Test

Since we are in the GLM framework, we can easily test for which model better fits the data using a Likelihood ratio test (LRT). The null hypothesis here is that the linear model is just as good as the larger (in terms of number of parameters) splines model.

```{r}
anova(casebase_time, casebase_splines, test = "LRT")
```

We see that splines model is the better fit.


## Session Information

```{r}
print(sessionInfo(), locale = F)
```

